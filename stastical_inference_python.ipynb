{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variablity in Point Estimates\n",
    "\n",
    "* Point Estimates are not exact\n",
    "* Sampling distribution: reperesents the distribution of the point estimates based on samples of a fixed size from a certain population. \n",
    "* Stadard Error of an estimate, $SE$: describes the typical error or uncertainty associated with the estimate. \n",
    "\n",
    "\n",
    "**Standard Error of the Mean**    \n",
    "Given $n$ independent observations from a population with standard deviation $\\sigma$ the standard error of the sample mean is equal to: \n",
    "\n",
    "$$\n",
    "SE = \\frac{\\sigma}{\\sqrt{n}}\\sim  \\frac{s}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "In detail:\n",
    "* In order to ensure that sample observations are **independent**, a reliable method is to conduct a simple random sample consisting of ***less than 10%*** of the population. \n",
    "* Population standard deviations $\\sigma$ is typically unknown, but using the point estimate of the standard deviation of the sample, $s$. \n",
    "    * This is sufficiently good when the ***sample size*** is ***greater than 30*** (or equal) AND the population distribution is ***not strongly skewed***. \n",
    "    * If the sample size $<30$, we need a method to account for extra uncerainty in the SE. \n",
    "    * If the skew condition is not met, a larger sample is needed to compesnate for the extra skew.\n",
    "\n",
    "\n",
    "**Central Limit Theorem**  \n",
    "If a sample consists of at least 30 independent observations and the data are not strongly skewed, then the distribution of the sample mean is well approximated by the normal model. \n",
    "\n",
    "**Confidence Intervals**  \n",
    "Plausable range of values of a population parameter.  \n",
    "\n",
    "$$\n",
    "\\text{point estimate} \\pm \\text{(no. of } SE\\text{s}) \\times SE\n",
    "$$\n",
    "\n",
    "A popular examples is $\\text{(no. of } SE\\text{s})\\approx2$ yields a Confidence Interval in which we have \n",
    "95% confidencde that the true value is within the range.  \n",
    "\n",
    "Correct interpretation:  \n",
    "> We are XX% confident that the poulation parameter is between ...\n",
    "\n",
    "Incorrect interpretations:  \n",
    "* It is incorrect to describe the CI as capturing the population parameter with certain probability. \n",
    "* The CI says nothing about the confidence of capturing individual observations, a proportion of the observations or about capturing point estimates.\n",
    "\n",
    "\n",
    "**Confidence Intervals For Normal Distributions**   \n",
    "For point estaimtes that follow the normal model:\n",
    "$$\n",
    "\\text{point estimate} \\pm z^* \\times SE\n",
    "$$\n",
    "* $ z^*$ - The Z-score is used to determine the confidence level selected. \n",
    "* $z^* \\times SE$ is called the *margin of error*.\n",
    "\n",
    "$ z^*$   | Confidence Level\n",
    "------------- | -------------\n",
    "1.96  | 95%\n",
    "2.58  | 99%\n",
    "\n",
    "Conditions to help ensure the sampling distribution is nearly normal and the estimate of $SE$ is sufficiently accurate:  \n",
    "* The sample of observations are independent\n",
    "* The sample size is large: $n\\ge 30$ is a good rule of thumb \n",
    "* The population distribution is not strongly skewed (difficult to evaluate, best to use judgement). The larger the sample size, the more lenient we can be with the sample's skew.   \n",
    "\n",
    "To verify sample observations are independent: \n",
    "* If the observations are from a simple random sample and consist of fewer than 10% of the population they may be considered independent. \n",
    "* Subjects in an experiment are considered independent if they undergo random assignment to the treatment groups. \n",
    "\n",
    "Checking for strong skew usually means checking for obvious outliers  \n",
    "* One needs at least 100 observations and in some cases much more. \n",
    "* Studentized bootstrap (bootstrap-t) method might be useful\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null Hypothesis Testing\n",
    "Also called *Classical Hypothesis Testing*. \n",
    "\n",
    "The *Null Hypothesis* $H_0$ often represents either a skeptical perspective of a claim to be tested (e.g no difference between samples).  \n",
    "The *Alteranative Hypothesis* $H_\\text{A}$ represents an alternative claim under consiersation. This is often represented by a range of possible parameter values.  \n",
    "\n",
    "General Tip of the Hypothesis testing framework:  \n",
    "The skeptic will not reject $H_0$ unless the evidence in favor of $H_A$ is very strong.  \n",
    "\n",
    "**Decision Errors** \n",
    "\n",
    "*Type 1* - Rejecting $H_0$ when it is true.   \n",
    "*Type 2* - Failing to reject $H_0$ when $H_\\text{A}$  is true.  \n",
    "\n",
    "The significance level $\\alpha$ should reflect the consequences associatd with Type 1 and Type 2 Errors.   \n",
    "E.g, if Type 1 Error is dangerous/costly, we might want to lower $\\alpha$.  \n",
    "If Type 2 Error is more dangerous/costly, we might want a higher $\\alpha$.  \n",
    "\n",
    "**Test Statistic**  \n",
    "A summary statistic that is useful for evaluating a hypothesis test or identifying the p-value.  \n",
    "For a point estimate that is nearly normal we use the Z-score.  \n",
    "\n",
    "\n",
    "Main disadvantage:  \n",
    "Does not incorporate prior knowledge.  \n",
    "\n",
    "## p-value\n",
    "A way of quantifying the strength of the evidence against the null hypothesis and in favor of the alternative.  \n",
    "\n",
    "It is the probability of getting a result at least as extreme as the observed if the null hypothesis (and all other modelling assumptions) were true. \n",
    "\n",
    "The p-value is used in the context of null hypothesis testing in order to quantify the idea of statistical significance of evidence.  \n",
    "A result is said to be *statistically significant* if it allows us to reject the null hypothesis.  \n",
    "\n",
    "$$\n",
    "\\text{right tail:   Pr}(X\\ge x|H_0) \\\\ \\text{left tail:   Pr}(X\\le x|H_0) \\\\\n",
    "\\text{double tail:   2min[Pr}(X\\ge x|H_0),\\text{Pr}(X\\le x|H_0)]\n",
    "$$\n",
    "* $X$ random variable of which we observed value $x \\in X$.   \n",
    "\n",
    "The null hypothesis $H_0$ is rejected if any of these probabilities is less than or equal to a small, fixed but arbitrarily pre-defined threshold value of level of significance, $\\alpha$.  \n",
    "\n",
    "The smaller the $p$-value the stronger the data favor $H_\\text{A}$ over $H_0$.   \n",
    "Or in other words: A small $p$-value means that if the null hypothesis is true, there is a low probability of seeing a point estimate at least as extereme as the one observed. This is interpreted as strong evidence in favor of $H_\\text{A}$.  \n",
    "\n",
    "We reject $H_0$ if the $p-$value is smaller than the significance level $\\alpha$. Otherwise we fail to reject $H_0$. \n",
    "\n",
    "The $p$-value is compared to the significance level $\\alpha$ in a way to ensure that the Type 1 Error rate does not exceed the signifiacne level standard.  \n",
    "\n",
    "**One-sided and two-sided tests**  \n",
    "Always use a two-sided test unless it was made clear prior to data collection that the test should be one-sided. Switching a two-sided test to a one-sided test after observing the data is dangerous because it can inflate the Type 1 Error rate.  \n",
    "\n",
    "Use a one-sided test when you are interested in checking for an increase of a decrease - but not both.  \n",
    "Use a two-sided test when you are interested in any difference from the null value.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\chi^2$ Test Of Association\n",
    "Useful for comparing counts of categorical data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T20:11:48.592520Z",
     "start_time": "2020-06-07T20:11:48.581877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed:[30. 14. 34. 45. 57. 20.]\n",
      "expected:[20. 20. 30. 40. 60. 30.]\n",
      "chi^2=11.44 of p=0.043 for 5 dof\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chisquare\n",
    "import numpy as np\n",
    "\n",
    "null_hypothesis = np.array([10., 10., 15., 20., 30., 15.])  # this is the frequencies that is hypothesised\n",
    "observed = np.array([30., 14, 34., 45., 57., 20]) # this is what we observe\n",
    "\n",
    "if null_hypothesis is None: # i.e, assuming even frequencies\n",
    "    null_hypothesis = np.array([1./len(observed)] * len(observed))\n",
    "    \n",
    "# prep\n",
    "expected = null_hypothesis * observed.sum()/null_hypothesis.sum()\n",
    "print(f'observed:{observed}')\n",
    "print(f'expected:{expected}')\n",
    "assert (expected >=5).sum() == len(expected) # large counts condition\n",
    "\n",
    "# calculations\n",
    "dof = len(null_hypothesis) - 1   # degrees of freedom. -1 b/c they are not indepenedent (the last one can be figured out from all the rest, due to its adding to 100%)\n",
    "chi2, pval = chisquare(observed, f_exp=expected)\n",
    "\n",
    "print(f'chi^2={chi2:0.2f} of p={pval:0.3f} for {dof} dof')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Types\n",
    "\n",
    "**False Positive** (Type 1 error) \n",
    "Falsely detecting a positive when it is actually a negative\n",
    "\n",
    "E.g, \n",
    "* The boy who cried wolf\n",
    "\n",
    "$\\alpha$ = False Positive Rate  \n",
    "Equal (or also referred to as):  \n",
    "* Significance Level\n",
    "* $\\alpha = 1 -$ specificity = $1 -$ TNR\n",
    "\n",
    "\n",
    "**False Negative** (Type 2 error)   \n",
    "Falsely detecting a negative when it is actually a positive\n",
    "\n",
    "E.g, \n",
    "* Important to minimise FNR for people who have cancer (we prefer high recall)\n",
    "* $\\alpha$ is a meausure Significance Level\n",
    "* $\\alpha = 1 -$ specificity = $1 -$ TNR\n",
    "\n",
    "$\\beta$ = False Negative Rate \n",
    "Equal (or also referred to as): \n",
    "* $1 - \\beta$ is a measure of Power\n",
    "* $\\beta = 1 -$ sensitivity = $1 -$ TFR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Power\n",
    "\n",
    "The power of a binary hypothesis test is the probability that the test correctly rejects the null hypothesis when a specific alternative hypothesis is true.\n",
    "Alternatively it is the probability of accepting the alternative hypothesis when it is true, i.e, the ability of a test to detect a specific effect that actually exists.\n",
    "\n",
    "Power $= 1 - \\beta = P(\\text{reject }H_0|H_A \\text{ is true} )$ \n",
    "\n",
    "\n",
    "**Power Analysis**  \n",
    "\n",
    "Effect Size $$d = \\frac{\\text{estimated difference beween means}}{\\text{pooled estimated standard deviations}}$$\n",
    "\n",
    "\n",
    "**Sample Size**\n",
    "\n",
    "Rule of thumb (I'm not sure of limitations ...)\n",
    "\n",
    "$$n=16\\frac{\\sigma^2}{d^2}$$\n",
    "\n",
    "* d - minimum effect we wish to detect\n",
    "* $\\sigma^2$ - sample variance expected. If not known for a binomial expeirement may use $\\sigma^2 = p(1-p)$\n",
    "\n",
    "**Rules of Thumb**\n",
    "\n",
    "* Don‚Äôt report significance levels until an experiment is over\n",
    "* Don't use significance levels to decide whether an experiment should stop or continue. \n",
    "\n",
    "Instead of reporting significance of ongoing experiments, report how large of an effect can be detected given the current sample size. That can be calculated with:\n",
    "\n",
    "$$d=(t_{\\alpha/2} + t_{\\beta} )\\sigma \\sqrt{2/n}$$\n",
    "\n",
    "Where the two ùë°‚Äôs are the t-statistics for a given significance level ùõº/2 and power (1‚àíùõΩ).\n",
    "\n",
    "**Useful Resources**  \n",
    "* [Evan Miller's: How Not to Run an A/B test](https://www.evanmiller.org/how-not-to-run-an-ab-test.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood Ratios\n",
    "[TBD](https://en.wikipedia.org/wiki/Likelihood_principle#The_law_of_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Hypothesis Testing\n",
    "\n",
    "## Bayes Factor\n",
    "[TBD](https://en.wikipedia.org/wiki/Bayes_factor)   \n",
    "Used in Bayesian model comparison. \n",
    "The aim of the Bayes factor is to quantify the support for a model over another, regardless of whether these models are correct.\n",
    "\n",
    "$$\n",
    "K = \\frac{\\text{Pr}(D|M_1)}{\\text{Pr}(D|M_2)}=\\frac{\\text{Pr}(M_1|D)\\text{Pr}(M_2)}{\\text{Pr}(M_2|D)\\text{Pr}(M_1)}\n",
    "$$\n",
    "\n",
    "Harold Jeffery's scale (TBD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "* OpenIntro Statistics, Diez, Barr, √áetinkaya-Rundel ([site](https://www.openintro.org/), [book](https://www.amazon.co.uk/OpenIntro-Statistics-Third-David-Diez/dp/194345003X), [coursera](https://www.coursera.org/learn/inferential-statistics-intro))\n",
    "* The Art of Statistics, Spiegelhalter ([book](https://www.amazon.co.uk/Art-Statistics-Learning-Pelican-Books/dp/0241398630))\n",
    "* Johan K. Kruschke\n",
    "    * Bayesian Estimation Supersedes the t Test ([YouTube](https://www.youtube.com/watch?v=fhw1j1Ru2i0), [pdf](https://pdfs.semanticscholar.org/dea6/0927efbd1f284b4132eae3461ea7ce0fb62a.pdf), [PyMC3](https://docs.pymc.io/notebooks/BEST.html))\n",
    "* Visual Website Optimizer\n",
    "    * [Bayesian A/B Testing at VWO](https://cdn2.hubspot.net/hubfs/310840/VWO_SmartStats_technical_whitepaper.pdf)\n",
    "    * [What Is Smart Decision?](https://help.vwo.com/hc/en-us/articles/360033991153-What-Is-Smart-Decision-)\n",
    "* Bayesian A/B Testing: a step-by-step guide ([blog](http://www.claudiobellei.com/2017/11/02/bayesian-AB-testing/), [GitHub](https://github.com/cbellei/abyes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
