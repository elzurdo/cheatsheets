{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Probability Graph Models\n",
    "\n",
    "**Undirected Graphical Model**   \n",
    "Also called: *Markov Random Field*.  \n",
    "No sense of direction between nodes \n",
    "$$\n",
    "G = (V,E) \\\\ \n",
    "p_{X_1...X_N}(x_1...x_n) = \\frac{1}{Z}\\prod_{i\\in V}\\phi_i(x_i)\\prod_{(i,j)\\in E}\\psi_{i,j}(x_i,x_j)\n",
    "$$\n",
    "\n",
    "* (this is actually an undirected pairwise graphical model)\n",
    "* $G(V,E)$ graph with nodes/vertices $V$ and edeges $E$ \n",
    "* $p_{X_1...X_N}(x_1...x_n)$ joint probability table of $G$.\n",
    "* $\\phi_i(x_i)$ singleton potential functions (marginal \"probablity\" of a node)\n",
    "* $\\psi_{i,j}(x_i,x_j)$ pairwise potential functions (joint \"probability\" of two nodes)\n",
    "* potential functions need to be non-negative but need not sum to 1.\n",
    "* $Z$ normalisation constant\n",
    "\n",
    "\n",
    "Two Fundamental Tasks in Graphical Models:  \n",
    "* Marginalization: Compute marginal probability table  $ğ‘_{ğ‘‹_ğ‘–}$  for every  $ğ‘–âˆˆğ‘‰$.\n",
    "* Most probable configuration: Compute the most probable configuration  $(\\hat{x}_1,\\hat{x}_2,â€¦,\\hat{x}_ğ‘›)$  such that\n",
    "\n",
    "$$\n",
    "(\\hat{x}_1,\\hat{x}_2,â€¦,\\hat{x}_ğ‘›) = \\text{argmax}_{ğ‘¥_1,ğ‘¥_2,â€¦,ğ‘¥_ğ‘›}  ğ‘_{ğ‘‹_1,ğ‘‹_2,â€¦,ğ‘‹_ğ‘›}(ğ‘¥_1,ğ‘¥_2,â€¦,ğ‘¥_ğ‘›)\n",
    "$$\n",
    "(Here we are assuming conditioning has already happened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum-Product Algorithm\n",
    "Also called *belief propagation*. \n",
    "\n",
    "Finds all the marginal probability distributions in any tree-structured undirected graphical model. \n",
    "\n",
    "This algorithm specifically works when the corresponding graph is a tree, which means that it has no loops and we can reach from any node in the graph to any other node by traversing along edges. \n",
    "\n",
    "1. Choose a root note (arbitrarily) and identify corresponding leaf nodes.\n",
    "\n",
    "2. Proceed from the leaf nodes to the root node computing required messages along the way.\n",
    "\n",
    "When the root node is reached, reverse direction and calculate messages that go back to the leaf nodes\n",
    "\n",
    "The equation for computing the table of messages is given by\n",
    "\n",
    "$$\n",
    "m_{i\\rightarrow j}(x_j) = \\sum_{x_i}\\left[\\phi_i\\left(x_i\\right)\\psi_{i,j}\\left(x_i, x_j \\right) \\prod_{k \\in \\mathcal{N}(i), k\\ne j}m_{k\\rightarrow i} (x_i) \\right]\n",
    "$$\n",
    "\n",
    "3. For each node $ğ‘–$, use the incoming messages to compute the marginal distribution $p_{X_i}$:\n",
    "$$\n",
    "p_{X_i}(x_i) = \\frac{1}{Z}\\phi_i(x_i)\\prod_{j\\in \\mathcal{N}(i)} m_{j \\rightarrow i}(x_i) = \\frac{1}{Z} \\tilde{p}_{X_i}(x_i)\n",
    "$$\n",
    "Typically, this marginal is computed by first computing the unnormalized table $\\tilde{p}_{X_i}(x_i)$  and then normalizing it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notation**  \n",
    "* $X:\\Omega \\rightarrow \\mathcal{X}$ means rand variable $X$ maps from sample space $\\Omega$ to alphabet $\\mathcal{X}$.  \n",
    "* alphabet - the possible values that a r.v can take. $|\\mathcal{X}|$ is the length of the alphabet.  \n",
    "* $\\mathbb{P}(X=x)$ probability that r.v $X$ is equal to value $x$.  \n",
    "* $\\{X=x\\}$ is short for $\\{\\omega\\in\\Omega: X(\\omega)=x\\}$  \n",
    "* $X\\sim N(\\mu,\\sigma)$ means r.v $X$ is distributed like ... (in this example Normal distribution)\n",
    "* $\\exists$ - exists\n",
    "* $\\mathcal{N}(i)$ denotes the neighboring nodes of node $ğ‘–$ in the graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "\n",
    "A special case of a tree-structured graphical model which uses a *forward-backward algorithm*.  \n",
    "\n",
    "* We observe  $ğ‘Œ_1$  through  $ğ‘Œ_ğ‘›$ , which we model as coming from hidden states  $ğ‘‹_1$  through  $ğ‘‹_ğ‘›$ \n",
    "\n",
    "* The goal of the forward-backward algorithm is to find the conditional distribution over hidden states given the data.\n",
    "\n",
    "* In order to specify an HMM, we need three quantities:\n",
    "    1. A **transition distribution**  $p_{X_{k+1}|X_k}(x_{k+1}|x_k)$ , which describes the distribution for the next state given the current state. This is often represented as a matrix, where the rows correspond to the current state, columns correspond to the next state, and each entry corresponds to the transition probability. That is, the entry at row $ğ‘–$  and column $j$ the $i,j$ element is $p_{X_{k+1}|X_k}(j|i)$.\n",
    "\n",
    "    2. An **observation distribution** (also called an *emission distribution*)  $p_{Y_k|X_k}(y_k|x_k)$ , which describes the distribution for the output given the current state. The rows correspond to the current state, and columns correspond to the observation. So, element $i,j$ is $p_{Y_k|X_k}(j|i)$: the probability of observing output $j$ from state $j$. Since the number of possible observations isn't necessarily the same as the number of possible states, this matrix  won't necessarily be square.\n",
    "\n",
    "    3. An **initial state distribution**  $p_{X_1}(x_1)$, which describes the starting distribution over states. The $i$th item in the vector represents $p_{X_1}(i)$.\n",
    "    \n",
    "*  Compute **forward and backwards messages** as follows:\n",
    "\n",
    "$$\n",
    "\\alpha_{(k-1) \\rightarrow k}(x_k) = \\sum_{x_{k-1}} \\overbrace{\\alpha_{(k-2) \\rightarrow (k-1)}(x_{k-1})}^\\textrm{previous message} \\overbrace{\\tilde{\\phi}(x_{k-1})}^\\textrm{observation} \\overbrace{\\psi(x_{k-1}, x_k)}^\\textrm{transition} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_{(k+1) \\rightarrow k}(x_k) = \\sum_{x_{k+1}} \\underbrace{\\beta_{(k+2) \\rightarrow (k+1)}(x_{k+1})}_\\textrm{previous message} \\underbrace{\\tilde{\\phi}(x_{k+1})}_\\textrm{observation} \\underbrace{\\psi(x_{k}, x_{k+1})}_\\textrm{transition} \n",
    "$$\n",
    "\n",
    "\n",
    "$\\tilde{\\phi}(x_k)=p_{Y_k|X_k}(y_k|x_k)$    \n",
    "$\\psi(x_{k}, x_{k+1})=p_{X_{k+1}|X_k}(x_{k+1}|x_k)$  \n",
    "$\\alpha_{0 \\rightarrow 1}(x_1)=p_{X_1}(x_1)$ - the first forward message.   \n",
    "$\\beta_{(n+1) \\rightarrow n}(x_n)$ is the first backward message and is initialised to uniform (i.e, equivalent to not including it at all). \n",
    "\n",
    "* To obtain a marginal distribution for a particular state given all the observations, $p_{X_k|Y_1...Y_n}$, we simply multiply the incoming messages together with the observation term, and then normalize\n",
    "\n",
    "$$\n",
    "p_{X_k|Y_1...Y_n} = \\alpha_{(k-1) \\rightarrow k}(x_k)\\beta_{(k+1) \\rightarrow k}(x_k)\\tilde{\\phi}(x_k)\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
