{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://avatars3.githubusercontent.com/u/7388996?s=400&v=4)\n",
    "\n",
    "![](http://www.codeinnovationsblog.com/wp-content/uploads/2016/02/python-development-services-india.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective \n",
    "\n",
    "January 27th 2018\n",
    "\n",
    "\n",
    "Here I provide code that I find useful for everyday practice in my work as a [data scientist](https://www.linkedin.com/in/eyal-kazin-0b96227a/).  \n",
    "This is a notebook in production, aimed at providing quick references to:    \n",
    "* [python](https://www.python.org/)/[Jupyter](http://jupyter.org/) basics  \n",
    "* [`pandas`](https://pandas.pydata.org/) with emphasis on data profiling and cleaning  \n",
    "* Machine Learning (mostly [`scikit-learn`](http://scikit-learn.org/stable/), but not limited to)  \n",
    "* plotting (mostly [matplotlib](https://matplotlib.org/) and [seaborn](https://seaborn.pydata.org/)) \n",
    "* Map making (`folium`)   \n",
    "* Statistics  \n",
    "\n",
    "\n",
    "It is in no way comprehensive, but rather for my personal use.  \n",
    "When possible, I try to put links to useful outside sources.   \n",
    "\n",
    "For people new to the data science in the python environment this might be useful to learn the playing ground,  \n",
    "where for more advanced it might serve as a few practical tips. \n",
    "\n",
    "With time I hope to have it a bit more wordy with explanations.   \n",
    "\n",
    "Cheers!  \n",
    "\n",
    "Eyal   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---- basics -------\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "# ---- plotting --------\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "dpi = 150 # 300\n",
    "mpl.rcParams['figure.dpi']= dpi\n",
    "\n",
    "# seaborn \n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "except:\n",
    "    None\n",
    "    \n",
    "# ----- pandas -----\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', 500)\n",
    "# pd.set_option('max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files/Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# appending path to PYTHONPATH\n",
    "import sys\n",
    "sys.path.append(\"/home/me/mypy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find all files with a given structure\n",
    "import glob\n",
    "file_format = \"./*.csv\"\n",
    "for file_temp in glob.glob(file_format):\n",
    "    print file_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickeling \n",
    "import pickle\n",
    "\n",
    "## Dumping\n",
    "pickle.dump( favorite_color, open( \"save.p\", \"wb\" ) )\n",
    "\n",
    "## Loading\n",
    "favorite_color = pickle.load( open( \"save.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Opening Excel (but see below for using Pandas)\n",
    "\n",
    "import xlwt # pip install xlwt\n",
    "\n",
    "def print_sheet(ws, values)\n",
    "  for irow, row in enumerate(values):\n",
    "      for icol, value in enumerate(row):\n",
    "          ws.write(irow, icol, value)\n",
    "\n",
    "wb = xlwt.Workbook()\n",
    "print_sheet(wb.add_sheet(\"1st result\"), df1.values)\n",
    "print_sheet(wb.add_sheet(\"2nd result\"), df2.values)\n",
    "wb.save(\"example_file.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data type handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `str`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zero buffering\n",
    "\"{:03d}\".format(x) # similar to \"%03d\"%x\n",
    "\n",
    "# adding ',' for every three digits (as in \"1,000\" instead of \"1000\")\n",
    "\"{:,}\".format(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inversing key-value relationships (only useful when values are unique)\n",
    "{v:k for k,v in dict_.iteritems()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrappers, Decorators\n",
    "\n",
    "A wrapper/decorator is a useful way to enhance the usage of a function by wrapping a wrapper function around it. [Useful tutorial](http://simeonfranklin.com/blog/2012/jul/1/python-decorators-in-12-steps/)\n",
    "\n",
    "Example:  \n",
    "In this example we will wrap a given function with a time_report function that report the time for that it takes the original function to execute.\n",
    "This is the time reporting function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the function \n",
    "import time\n",
    "\n",
    "def time_report(t0):\n",
    "    tseconds = time.time() - t0\n",
    "    seconds = \"%0.1f\" % tseconds\n",
    "    minutes = \"%0.1f\" % (tseconds / 60.)\n",
    "    hours = \"%0.2f\" % (tseconds / 3600.)\n",
    "    print \"Time s:{}, m:{}, h:{}\".format(seconds, minutes, hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the wrapper\n",
    "\n",
    "def time_report_wrapper(func):\n",
    "    def inner(*args,**kwargs):\n",
    "        t_start = time.time()\n",
    "        #print \"Arguments were: {}, {}\".format(args, kwargs)\n",
    "        result = func(*args,**kwargs)\n",
    "        time_report(t_start)\n",
    "        return result\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two option to wrap `time_report_wrapper` around `your_function`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can either do:\n",
    "def your_function():\n",
    "    {awesome code here}\n",
    "\n",
    "your_function = time_report_wrapper(your_function)\n",
    "# This is done only after you defined your_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Or use the decorator symbol (as of python 2.4)\n",
    "@time_report_wrapper\n",
    "def your_function():\n",
    "    {awesome code here}\n",
    "# I.e, you just \"decorate\" your_function with the wrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping  \n",
    "Adding arguement to a map by creating new function.  \n",
    "E.g assuming a function `mapfunc` that takes an argument `myarg`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "mapfunc = partial(my_function, myarg=myarg)\n",
    "map(mapfunc, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "whatever = deepcopy(whatever_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running**   \n",
    "On bash line `jupyter-notebook`  \n",
    "or if you have a port number in mind (e.g 9039):   \n",
    "`jupyter-notebook --port=9039`     \n",
    "\n",
    "**Extensions**  \n",
    "See [`jupyter_contrib_nbextensions`](https://github.com/ipython-contrib/jupyter_contrib_nbextensions)  \n",
    "\n",
    "**Embedding Image**    \n",
    "In Markdown mode:  \n",
    "`![title](./example_graph.png)`    \n",
    "\n",
    "[Tutorial with advanced tips/tricks](https://blog.dominodatalab.com/lesser-known-ways-of-using-notebooks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas  \n",
    "[Tutorial: pandas in 10 minutes](http://pandas.pydata.org/pandas-docs/stable/10min.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading\n",
    "xlsx = pd.ExcelFile(\"file.xlsx\")\n",
    "\n",
    "print xlsx.sheet_names\n",
    "df = xlsx.parse(xlsx.sheet_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# writing\n",
    "writer = pd.ExcelWriter(\"example_file.xls\")\n",
    "df1.to_excel(writer, sheet_name=\"1st result\")\n",
    "df2.to_excel(writer, sheet_name=\"2nd result\")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_meta = pd.DataFrame({'completes': df.notnull().sum(), 'completes_%':df.notnull().sum() * 100./ df.shape[0]}).loc[df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### [`pandas_profiling`](https://github.com/pandas-profiling/pandas-profiling/blob/master/examples/meteorites.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas_profiling  # pip install pandas-profiling\n",
    "pandas_profiling.ProfileReport(df)\n",
    "\n",
    "pfr = pandas_profiling.ProfileReport(df)\n",
    "pfr.to_file(\"./example.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_order = [df.index.name] + df.columns.tolist() # assuming the index.name is not null\n",
    "df_meta = pfr.description_set['variables'].loc[l_order] # is a DF with MetaData\n",
    "\n",
    "print df_meta.shape\n",
    "df_meta.head(4)\n",
    "\n",
    "#E.g, pfr.description_set['variables']['type'] is the columns type:\n",
    "#Numeric\n",
    "#Categorical\n",
    "#Boolean\n",
    "#Date\n",
    "#Text (Unique)\n",
    "#Rejected\n",
    "#Unsupported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# useful to determine ordinal features examining the number of distinct \n",
    "l_numerical = df_meta[df_meta['type'] == 'NUM'].index.tolist()\n",
    "print df_meta[df_meta['type'] == 'NUM']['distinct_count'].sort_values()\n",
    "\n",
    "\n",
    "# useful to determine binary/multicategorical features examining the number of distinct \n",
    "sr_distCounts =  df_meta[df_meta['type'] == 'CAT']['distinct_count'].sort_values()\n",
    "print sr_distCounts.head(6)\n",
    "\n",
    "\n",
    "def meta_to_types(meta, data):\n",
    "    # distinguishes between the types: unary, binary, mutlicategorical\n",
    "    sr_distCounts =  meta[meta['type'] == 'CAT']['distinct_count'].sort_values()\n",
    "\n",
    "    l_unary = []\n",
    "    l_binary = []\n",
    "    for col, distinct in sr_distCounts.iteritems():\n",
    "        # because the profiler considers NaN as a distinct entry this code is more cautious \n",
    "        if distinct <= 3: # extereme case 3 values including NaN\n",
    "            distinct_notNan = len(data[col].value_counts(dropna=True))\n",
    "            if distinct_notNan == 1: # Unary\n",
    "                l_unary.append(col)\n",
    "                print col, distinct_notNan, \"Unary\"\n",
    "            elif distinct_notNan == 2: # Binary\n",
    "                l_binary.append(col)\n",
    "                print col, distinct_notNan,\"Binary\"\n",
    "\n",
    "    l_multiCategorical = list( (set(sr_distCounts.index.tolist()) -  set(l_binary) ) - set(l_unary)  )   \n",
    "\n",
    "    return l_unary, l_binary, l_multiCategorical\n",
    "\n",
    "l_unary, l_binary, l_multiCategorical = meta_to_types(df_meta, df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ordinals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----- \n",
    "\n",
    "# Does not currently support type = \n",
    "#Boolean -- need to integrate with 'binary'\n",
    "#Date\n",
    "#Text (Unique)\n",
    "#Rejected\n",
    "#Unsupported\n",
    "\n",
    "# once looked at and understood, convert to ordinal\n",
    "l_ordinal = ['OverallCond', 'OverallQual']\n",
    "\n",
    "print len(l_binary), len(l_multiCategorical), len(l_numerical)\n",
    "\n",
    "l_binary = list( set(l_binary) - set(l_ordinal))\n",
    "l_multiCategorical = list( set(l_multiCategorical) - set(l_ordinal))\n",
    "l_numerical =  list( set(l_numerical) - set(l_ordinal))\n",
    "\n",
    "print len(l_binary), len(l_multiCategorical), len(l_numerical), len(l_ordinal)\n",
    "\n",
    "\n",
    "# ------ creating new df_metaF (as in final) ---------\n",
    "df_metaF = df_meta.copy()\n",
    "\n",
    "df_metaF.loc[l_unary, 'data_type'] = 'unary'\n",
    "df_metaF.loc[l_binary, 'data_type'] = 'binary'\n",
    "df_metaF.loc[l_ordinal, 'data_type'] = 'ordinal'\n",
    "df_metaF.loc[l_multiCategorical, 'data_type'] = 'multiCategorical'\n",
    "df_metaF.loc[l_numerical, 'data_type'] = 'numerical'\n",
    "\n",
    "print df_metaF.shape\n",
    "print df_metaF['type'].value_counts()\n",
    "print '-' * 20\n",
    "print df_metaF['data_type'].value_counts()\n",
    "\n",
    "df_metaF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing on more complete entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# counts of special values overall\n",
    "print df_meta[['n_infinite', 'n_missing', 'n_zeros']].sum()\n",
    "\n",
    "# percentages of potentially missing data per column\n",
    "def print_incompleteness(meta, size, col_sort = 'n_zeros', ascending=False, normalise=True):\n",
    "    df_ = meta[['n_infinite', 'n_missing', 'n_zeros']]\n",
    "    if normalise:\n",
    "        df_ = df_ * 100. / size\n",
    "    print (df_ ).sort_values(col_sort, ascending=ascending)\n",
    "    \n",
    "print_incompleteness(df_meta, df.shape[0], normalise=False, col_sort='n_missing') # 'n_zeros' 'n_missing' 'n_infinite'\n",
    "\n",
    "# lists of column that have many missing data (consider not using in first iteration)\n",
    "# ALSO consider creating a binary feature of each (as in True: has value, False: Missing)\n",
    "missing_thresh = 0.15\n",
    "l_missingHigh = df_meta[df_meta['n_missing'] > missing_thresh].index.tolist()\n",
    "\n",
    "zeros_thresh = 0.15\n",
    "l_zerosHigh = df_meta[df_meta['n_zeros'] > zeros_thresh].index.tolist()\n",
    "\n",
    "\n",
    "# ---------- Focusing on a Subset of Features ------------- \n",
    "l_explore = list( set(df_train.columns.tolist()) - set(l_missingHigh) - set(l_zerosHigh))\n",
    "\n",
    "print df_train.shape\n",
    "df_explore = df_train[l_explore]\n",
    "print df_explore.shape\n",
    "\n",
    "print df_metaF.loc[df_explore.columns, 'data_type'].value_counts()\n",
    "print_incompleteness(df_metaF.loc[df_explore.columns], df_train.shape[0], normalise=False, col_sort='n_missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing \n",
    "currently focusing on the probabilistic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_nonunary =  list( set(df_explore.tolist()) -  set(df_metaF.loc[df_explore.columns][df_metaF.loc[df_explore.columns, 'data_type'] == 'unary'].index) )\n",
    "\n",
    "imputing_method = 'probabilstic' # should cover all types \n",
    "l_Zerospecial = [] #special treaement - NaNs get zero\n",
    "\n",
    "\n",
    "# other methods: mean, median, probabilistic\n",
    "for col in l_nonunary:\n",
    "    bool_ = (df_explore[col].isnull()  )  # |  df_explore[col].map(np.isinf) \n",
    "    idx = df_explore[bool_].index\n",
    "    idx_use = df_explore[~bool_].index\n",
    "    \n",
    "    if col in l_Zerospecial:\n",
    "        df_explore.loc[idx, col] = 0\n",
    "    else:\n",
    "        df_explore.loc[idx, col] = pd.Series(df_explore.loc[idx_use, col].sample(n=len(idx), replace=True).tolist(), index=idx)\n",
    "    \n",
    "    print col, len(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_binary_explore = [col  for col in l_binary if col in df_explore.columns.tolist()]\n",
    "l_multiCategorical_explore = [col  for col in l_multiCategorical if col in df_explore.columns.tolist()]\n",
    "\n",
    "l_categorical_explore = l_binary_explore + l_multiCategorical_explore\n",
    "\n",
    "df_dummies = df_explore[[]]\n",
    "for col in l_categorical_explore:\n",
    "    \n",
    "    df_dummies_temp = pd.get_dummies(df_explore[col])\n",
    "    df_dummies_temp.columns = df_dummies_temp.columns.map(lambda x: \"{}_{}\".format(col, x))\n",
    "    \n",
    "    if col in l_binary:\n",
    "        # dropping second column\n",
    "        l_cols_dummy = sorted(df_dummies_temp.columns) #sorted(df_training[col].unique())\n",
    "        df_dummies_temp.drop(l_cols_dummy[-1], axis=1, inplace=True)\n",
    "    \n",
    "    df_dummies = df_dummies.join(df_dummies_temp)\n",
    "    \n",
    "    print col, df_dummies.shape\n",
    "  \n",
    "print df_dummies.shape\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Describing the standard deviation of each dummy variable, and selecting the important ones\n",
    "sr_std_dummies = df_dummies.describe().loc['std'].sort_values()\n",
    "\n",
    "print sr_std_dummies.describe()\n",
    "\n",
    "# ===========\n",
    "std_thresh = 0.2\n",
    "\n",
    "l_dummies_use = sr_std_dummies[sr_std_dummies >= std_thresh].index\n",
    "for col in sorted(l_dummies_use):\n",
    "    print col\n",
    "print df_dummies[l_dummies_use].shape\n",
    "df_dummies[l_dummies_use].head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print df_explore.shape\n",
    "df_explore = df_explore.drop(l_categorical_explore, axis=1).join(df_dummies[l_dummies_use])\n",
    "\n",
    "print df_explore.shape\n",
    "df_explore.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select numerical column columns \n",
    "\"\"\"\n",
    "bool_ = df_meta['type'] == 'NUM'\n",
    "\n",
    "# might want a minimu value threshold\n",
    "numerical_thresh = 10\n",
    "bool_ &= df_meta['distinct_count'] > numerical_thresh \n",
    "\n",
    "l_col_manyNum = df_meta[bool_].index.tolist()\n",
    "\n",
    "# might want to exclude column with many missing data\n",
    "l_col_manyNum = list(set(l_col_manyNum) - set(l_missingHigh) - set(l_zerosHigh))\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using visual aid. l_col_manyNum is defined above\n",
    "l_numerical_explore = [col  for col in l_numerical if col in df_explore.columns.tolist()]\n",
    "\n",
    "npanels = len(l_numerical_explore)\n",
    "\n",
    "ncols = 4\n",
    "nrows = npanels / ncols + np.sum( (npanels % ncols) != 0 )\n",
    "\n",
    "width, height = 3, 4 \n",
    "plt.figure(figsize=(width*ncols, height*nrows))\n",
    "\n",
    "for panel, col in enumerate(l_numerical_explore):\n",
    "    plt.subplot(nrows, ncols, panel + 1)\n",
    "    values = df[col][df[col].notnull()]\n",
    "    \n",
    "    plt.hist(values)\n",
    "    plt.title(col, fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the columns that should be logarithmic\n",
    "l_toLog = ['LotArea', 'SalePrice']\n",
    "print df_explore[l_toLog].describe(percentiles=np.arange(0., 1.1, 0.1)).T[['10%', '90%']]\n",
    "\n",
    "\n",
    "# creating logarithmic DF\n",
    "df_logs = df_train[[]]\n",
    "for col in l_toLog:\n",
    "    print col\n",
    "    \n",
    "    df_logs = df_logs.join(df[col].map(np.log10))\n",
    "  \n",
    "df_logs.columns = df_logs.columns.map(lambda x: \"{}_log10\".format(x))\n",
    "print df_logs.shape\n",
    "df_logs.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# joining with previous data, and updating the list of columns (df_meta not updated!)\n",
    "df = df.drop(l_toLog, axis=1).join(df_logs)\n",
    "\n",
    "l_numerical_explore = list(set(l_numerical_explore) - set(l_toLog)) + df_logs.columns.tolist()\n",
    "\n",
    "# now use the visualisation from before again to see if that made sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_standard = df.copy()\n",
    "\n",
    "l_numerical_standard = []\n",
    "\n",
    "for col in l_numerical_explore:\n",
    "    mu = df_standard[col].mean()\n",
    "    sigma = df_standard[col].std()\n",
    "    \n",
    "    \n",
    "    col_standard = \"{}_standard\".format(col)\n",
    "    df_standard[col_standard] = (df_standard[col] - mu) / sigma\n",
    "    df_standard.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    l_numerical_standard.append(col_standard)\n",
    "    \n",
    "l_numerical_explore = list(l_numerical_standard)\n",
    "print df_standard[l_numerical_explore].shape\n",
    "df_standard[l_numerical_explore].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# correlations\n",
    "\n",
    "\n",
    "col_target = 'SalePrice_log10_standard'\n",
    "\n",
    "df_corrPearson = df_standard[l_numerical_explore].corr(method='pearson')[[col_target]]\n",
    "df_corrSpearman = df_standard[l_numerical_explore].corr(method='spearman')[[col_target]]\n",
    "\n",
    "df_corr_target = 100. * (df_corrPearson).join(df_corrSpearman, lsuffix='_pearson', rsuffix='_spearman')\n",
    "df_corr_target.drop(col_target, axis=0, inplace=True)\n",
    "df_corr_target['spearman_minus_pearson'] = df_corr_target[df_corr_target.columns[1]] - df_corr_target[df_corr_target.columns[0]]\n",
    "df_corr_target.sort_values('spearman_minus_pearson', ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# all numerical vs. all numerical\n",
    "df_corrPearson = pfr.description_set['correlations']['pearson']\n",
    "df_corrSpearman = pfr.description_set['correlations']['spearman']\n",
    "\n",
    "# all numerical vs. target numerical\n",
    "# spearman is less sensitive than pearson to outliers, and hence will show strong correlations and anticorrelations\n",
    "# hence the difference between them should indicate that the feature has outliers.\n",
    "col_target = 'SalePrice'\n",
    "\n",
    "df_corr_target = 100. * (df_corrPearson.loc[[col_target]].T).join(df_corrSpearman.loc[[col_target]].T, lsuffix='_pearson', rsuffix='_spearman')\n",
    "df_corr_target.drop(col_target, axis=0, inplace=True)\n",
    "df_corr_target['spearman_minus_pearson'] = df_corr_target[df_corr_target.columns[1]] - df_corr_target[df_corr_target.columns[0]]\n",
    "df_corr_target.sort_values('spearman_minus_pearson', ascending=False, inplace=True)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date-Time Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(df['DOB'], format=\"%Y/%m/%d\", errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dealing with hours\n",
    "pd.to_datetime(\"1:30 AM\", format=\"%I:%M %p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Categorical Data (Nominal Data)](https://en.wikipedia.org/wiki/Level_of_measurement#Nominal_level)\n",
    "\n",
    "> The nominal type differentiates between items or subjects based only on their names or (meta-)categories and other qualitative classifications they belong to; thus dichotomous data involves the construction of classifications as well as the classification of items. Discovery of an exception to a classification can be viewed as progress. Numbers may be used to represent the variables but the numbers do not have numerical value or relationship  \n",
    "\n",
    "E.g, gender, nationality, ethnicity, language, genre, style, biological species, and form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# binning numerical to categorical. E.g, Age to Age_brackets\n",
    "\n",
    "dict_dict_min, dict_dict_max = OrderedDict(), OrderedDict()\n",
    "\n",
    "col = 'Age'\n",
    "dict_dict_min[col], dict_dict_max[col] = OrderedDict(), OrderedDict()\n",
    "dict_dict_min[col]['18-34'] = 0. \n",
    "dict_dict_min[col]['35-49'] = 35.\n",
    "dict_dict_min[col]['50-64'] = 50.\n",
    "\n",
    "\"\"\"\n",
    "# or more generically \n",
    "dict_dict_min[col]['18-24'] = 0. # special - 18-24\n",
    "for minval in np.arange(25, 65, 5): # generic - leaps of 5 years\n",
    "    key = \"{}-{}\".format(str(minval), str(minval + 4))\n",
    "    dict_dict_min[col][key] = float(minval)\n",
    "dict_dict_min[col]['65+'] = 65. # special - 65 and over\n",
    "\"\"\"\n",
    "\n",
    "dict_dict_min[col]['65+'] = 65.\n",
    "\n",
    "keys = dict_dict_min[col].keys()\n",
    "for ikey, key in enumerate(keys[:-1]):\n",
    "    dict_dict_max[col][key] = dict_dict_min[col][keys[ikey + 1]]\n",
    "dict_dict_max[col][keys[-1]] = 20000.\n",
    "\n",
    "def numeric2brackets(df, col, col_bracket=None):\n",
    "    if not col_bracket:\n",
    "        col_bracket = \"{}_bracket\".format(col)\n",
    "    \n",
    "    dict_min = dict_dict_min[col]\n",
    "    dict_max = dict_dict_max[col]\n",
    "    \n",
    "    for key in dict_min.keys():\n",
    "        print key, dict_min[key], dict_max[key]\n",
    "        indexes_temp = df[(df[col] >= dict_min[key]) & (df[col] < dict_max[key])].index\n",
    "        df.loc[indexes_temp, col_bracket] = key\n",
    "    print \"-----\"\n",
    "    print df[col_bracket].value_counts(dropna=False, normalize=True) \n",
    "    \n",
    "numeric2brackets(df_, 'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Ordinal Data](https://en.wikipedia.org/wiki/Ordinal_data) \n",
    "> Ordinal data is a categorical, statistical data type where the variables have natural, ***ordered categories and the distances between the categories is not known***. The ordinal scale is distinguished from the nominal scale by having ordered categories. It also differs from interval and ratio scales by not having category widths that represent equal increments of the underlying attribute.\n",
    "\n",
    "Examples:  \n",
    "\n",
    "Likert scale:   \n",
    "Like=1\tLike Somewhat=2\tNeutral=3\tDislike Somewhat=4   Dislike=5  \n",
    "\n",
    "Income groupings \\$0-\\$19,999, \\$20,000-\\$39,999, \\$40,000-\\$59,999, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Categorical to Ordinal\n",
    "l_order = dict_dict_min['Age'].keys() # e.g ['18-34', '35-54', '55-64', '65+']\n",
    "df_.loc[:, 'Age_bracket'] = pd.Categorical(df_['Age_bracket'], categories=l_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grouping and yielding by size\n",
    "l_cols = ['Age_bracket', 'Gender']\n",
    "df_.groupby(l_cols).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grouping and yielding by percentage within group.\n",
    "l_cols = ['Age_bracket', 'Gender']\n",
    "df_.groupby(l_cols).size().groupby(level=0).apply(lambda x: x * 100./ x.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Highlighting `DataFrame`](http://pandas.pydata.org/pandas-docs/stable/style.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Linear Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "clf_linear = LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=-1)\n",
    "clf_linear.fit(df_X.values, df_y['true'].values)\n",
    "\n",
    "# Coefficients\n",
    "print pd.Series(clf_linear.coef_, index=df_X.columns)\n",
    "\n",
    "# score (coefficient of determination)\n",
    "# R^2 is defined as (1 - u/v)\n",
    "# u = ((y_true - y_pred) ** 2).sum()\n",
    "# v = ((y_true - y_true.mean()) ** 2).sum()\n",
    "clf_linear.score(df_XCV.values, df_yCV['true'].values)\n",
    "\n",
    "# predictions\n",
    "df_y['prediction'] = pd.Series(clf_linear.predict(df_X), index=df_y.index)\n",
    "\n",
    "# plotting \n",
    "plt.scatter(df_y['true'], df_y['prediction'], s=5)\n",
    "\n",
    "min_, max_ = df_y['true'].min(), df_y['true'].max()\n",
    "plt.plot([min_, max_], [min_, max_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desicion Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining a Tree\n",
    "[with scikit-learn](http://scikit-learn.org/stable/modules/tree.html#tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given a \n",
    "#clf = tree.DecisionTreeClassifier, \n",
    "#      tree.DecisionTreeRegressor,\n",
    "#      tree.ExtraTreeClassifier,\n",
    "#      tree.ExtraTreeRegressor\n",
    "from sklearn import tree\n",
    "import graphviz # pip install graphviz\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"iris\") # produces iris.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# more options (labeling, coloring)\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                     feature_names=iris.feature_names,  \n",
    "                     class_names=iris.target_names,  \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph.render(\"iris\") # produces iris.pdf\n",
    "graph # will show the tree in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_standard - assuming all features are numerical and standardised\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# exploring the variance expected to maintain\n",
    "n_components = 21 # number of compenents depending on the amount of variance to preserve\n",
    "\n",
    "print df_standard.shape\n",
    "random_state = 1\n",
    "pca = PCA(n_components=n_components, random_state=random_state)\n",
    "pca.fit(df_standard.values)\n",
    "\n",
    "print pca.explained_variance_ratio_ * 100., np.sum(pca.explained_variance_ratio_) * 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training set in terms of PCA eigen vectors\n",
    "print df_standard.shape\n",
    "df_training_pca = pd.DataFrame(pca.transform(df_standard.values), index=df_standard.index)\n",
    "df_training_pca.columns = df_training_pca.columns.map(lambda x: \"PCA_{}\".format(x))\n",
    "\n",
    "print df_training_pca.shape\n",
    "\n",
    "df_training_pca.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Examining the components themselves \n",
    "df_components = pd.DataFrame(pca.components_, index=df_training_pca.columns, columns=df_standard.columns)\n",
    "\n",
    "print df_components.shape\n",
    "df_components\n",
    "\n",
    "\n",
    "# ============= plotting all components (sorted by importance of one)\n",
    "# notice that here I use np.abs\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "sort_by = 'PCA_0'\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "ax = sns.heatmap(df_components.T.apply(np.abs).sort_values(sort_by) , annot=False, fmt=\"0.1f\", cmap='viridis')\n",
    "\n",
    "# ============ Examining one component\n",
    "pca_component = 'PCA_0'\n",
    "sr_component = df_components.loc[pca_component].map(np.abs).sort_values(ascending=False)\n",
    "sr_plot = df_components.loc[pca_component].loc[sr_component[sr_component > 0.1].index]\n",
    "sr_plot\n",
    "\n",
    "# ============ Scatter plot of entries \n",
    "plt.scatter(df_training_pca['PCA_0'], df_training_pca['PCA_1'], s=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assumes df_training_pca is all numerical (and possibly PCA after standardising)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "max_iter = 500\n",
    "\n",
    "n_clusters = 4 # change according to Knee bend\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, max_iter=max_iter).fit(df_training_pca.values)\n",
    "print kmeans.n_iter_, \" Iterations in practice\"\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Examining results \n",
    "colT = 'segment'\n",
    "\n",
    "df_results = df_training_pca.copy()\n",
    "df_results[colT] = pd.Series(kmeans.labels_, index=df_standard.index)\n",
    "\n",
    "print df_results.shape\n",
    "df_results.head(4)\n",
    "\n",
    "# ======= plotting =======\n",
    "l_cols_plot = ['PCA_0', 'PCA_1', 'PCA_2', 'PCA_3']\n",
    "\n",
    "\n",
    "df_plot = df_results[l_cols_plot + [colT]] #.sample(10000)\n",
    "\n",
    "g = sns.pairplot(df_plot, hue=colT, vars=l_cols_plot, size=5., diag_kws={\"alpha\": 0.7, 'histtype':'step', 'linewidth':4.}, plot_kws={\"alpha\": 0.4, \"s\":10}) #, plot_kws={\"alpha\": '0.7'})\n",
    "for i, j in zip(*np.triu_indices_from(g.axes, 1)):\n",
    "    g.axes[i, j].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tagging the segments by segment number\n",
    "df_segment = pd.DataFrame(pd.Series(kmeans.labels_, index=df_standard.index), columns=['segment_number'])\n",
    "print df_segment['segment_number'].value_counts(normalize=True)\n",
    "print df_segment.shape\n",
    "df_segment.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Knee curve\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "X = df_training_pca.values.copy()\n",
    "\n",
    "distortions = []\n",
    "K = range(1,20)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k, max_iter=max_iter).fit(X)\n",
    "    print k, kmeanModel.n_iter_\n",
    "    kmeanModel.fit(X)\n",
    "    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'b-o')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [BayesSearchCV](skopt.BayesSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv = 5\n",
    "scoring = None\n",
    "scores = cross_validate(clf, df_X.values, df_y['true'], scoring=scoring,\n",
    "                        cv=cv, return_train_score=True, n_jobs=-1)\n",
    "df_scores = pd.DataFrame(scores)\n",
    "df_scores.index.name = 'iteration'\n",
    "\n",
    "print df_scores.shape\n",
    "df_scores.head(4)\n",
    "\n",
    "# ======== examining scores ======== \n",
    "print \"{:0.3f} mean, {:0.3f} std Test Score\".format(df_scores['test_score'].mean(), df_scores['test_score'].var())\n",
    "print \"{:0.3f} mean, {:0.3f} std Train Score\".format(df_scores['train_score'].mean(), df_scores['train_score'].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# improving quality of plots \n",
    "\n",
    "import matplotlib as mpl\n",
    "dpi = 150 # 300\n",
    "mpl.rcParams['figure.dpi']= dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subplotting in many panels\n",
    "npanels = 7 \n",
    "\n",
    "ncols = 4\n",
    "nrows = npanels / ncols + np.sum( (npanels % ncols) != 0 )\n",
    "\n",
    "width, height = 6, 8 \n",
    "plt.figure(figsize= (width*ncols, height*nrows))\n",
    "for panel in range(npanels):\n",
    "    plt.subplot(nrows, ncols, panel + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `seaborn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seaborn \n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangular Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting for correlations\n",
    "corr = df_training[l_numerical].corr()\n",
    "corr = corr.applymap(np.abs)\n",
    "\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, center=0,annot=True, \n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Making\n",
    "\n",
    "## `folium`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import folium  # to install: pip install folium\n",
    "from folium import plugins\n",
    "\n",
    "folium.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initiate map (where and how deep, higher zoom_start is more zoomed in)\n",
    "lat_long = (35.1264416, 33.3309026)\n",
    "zoom_start = 13# higher values is more zoomed in\n",
    "\n",
    "map_ = folium.Map(location=lat_long, zoom_start=zoom_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# heatmap layer\n",
    "heatmap = plugins.HeatMap([[row['latitude'], row['longitude']] for name, row in df_geo.iterrows()], name = \"Heatmap\")\n",
    "map_.add_child(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# marker layer\n",
    "feature_group_centers = folium.FeatureGroup(name='Centers')\n",
    "\n",
    "for idx, row in df_plot.iterrows():\n",
    "    # we can label each label with popup \n",
    "    label = \"Center #{} \".format(idx)\n",
    "    folium.Marker([row['latitude'], row['longitude']], popup=label).add_to(feature_group_centers)\n",
    "    \n",
    "feature_group_centers.add_to(map_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cluster marker layer\n",
    "\n",
    "feature_group = plugins.MarkerCluster(name=\"Clustering Markers\")\n",
    "\n",
    "for idx, row in df_geo.iterrows():\n",
    "    label = u\"{}\".format(idx)\n",
    "    folium.Marker([row['latitude'], row['longitude']], popup=label).add_to(feature_group)\n",
    "\n",
    "map_.add_child(feature_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer control\n",
    "map_.add_child(folium.LayerControl(collapsed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to save\n",
    "map_.save('./maps/test.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `scipy`  \n",
    "## `stats`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "r, pval = pearsonr(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "308px",
    "left": "0px",
    "right": "1097.1px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
