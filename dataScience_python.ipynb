{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://avatars3.githubusercontent.com/u/7388996?s=400&v=4)\n",
    "\n",
    "![](http://www.codeinnovationsblog.com/wp-content/uploads/2016/02/python-development-services-india.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective \n",
    "\n",
    "January 27th 2018\n",
    "\n",
    "\n",
    "Here I provide code that I find useful for everyday practice in my work as a [data scientist](https://www.linkedin.com/in/eyal-kazin-0b96227a/).  \n",
    "This is a notebook in production, aimed at providing quick references to:    \n",
    "* [python](https://www.python.org/)/[Jupyter](http://jupyter.org/) basics  \n",
    "* [`pandas`](https://pandas.pydata.org/) with emphasis on data profiling and cleaning  \n",
    "* Machine Learning (mostly [`scikit-learn`](http://scikit-learn.org/stable/), but not limited to)  \n",
    "* plotting (mostly [matplotlib](https://matplotlib.org/) and [seaborn](https://seaborn.pydata.org/)) \n",
    "* Map making (`folium`)   \n",
    "* Statistics  \n",
    "\n",
    "\n",
    "It is in no way comprehensive, but rather for my personal use.  \n",
    "When possible, I try to put links to useful outside sources.   \n",
    "\n",
    "For people new to the data science in the python environment this might be useful to learn the playing ground,  \n",
    "where for more advanced it might serve as a few practical tips. \n",
    "\n",
    "With time I hope to have it a bit more wordy with explanations.   \n",
    "\n",
    "Cheers!  \n",
    "\n",
    "Eyal   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---- basics -------\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "# ---- plotting --------\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "dpi = 150 # 300\n",
    "mpl.rcParams['figure.dpi']= dpi\n",
    "\n",
    "# seaborn \n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "except:\n",
    "    None\n",
    "    \n",
    "# ----- pandas -----\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', 500)\n",
    "# pd.set_option('max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files/Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# appending path to PYTHONPATH\n",
    "import sys\n",
    "sys.path.append(\"/home/me/mypy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find all files with a given structure\n",
    "import glob\n",
    "file_format = \"./*.csv\"\n",
    "for file_temp in glob.glob(file_format):\n",
    "    print file_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickeling \n",
    "import pickle\n",
    "\n",
    "## Dumping\n",
    "pickle.dump( favorite_color, open( \"save.p\", \"wb\" ) )\n",
    "\n",
    "## Loading\n",
    "favorite_color = pickle.load( open( \"save.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Opening Excel (but see below for using Pandas)\n",
    "\n",
    "import xlwt # pip install xlwt\n",
    "\n",
    "def print_sheet(ws, values)\n",
    "  for irow, row in enumerate(values):\n",
    "      for icol, value in enumerate(row):\n",
    "          ws.write(irow, icol, value)\n",
    "\n",
    "wb = xlwt.Workbook()\n",
    "print_sheet(wb.add_sheet(\"1st result\"), df1.values)\n",
    "print_sheet(wb.add_sheet(\"2nd result\"), df2.values)\n",
    "wb.save(\"example_file.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data type handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `str`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zero buffering\n",
    "\"{:03d}\".format(x) # similar to \"%03d\"%x\n",
    "\n",
    "# adding ',' for every three digits (as in \"1,000\" instead of \"1000\")\n",
    "\"{:,}\".format(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_ = ['a', 'b'] # or list()\n",
    "\n",
    "# appending item\n",
    "l_.append('c') # ['a', 'b', 'c']\n",
    "\n",
    "# inserting item\n",
    "idx = 2\n",
    "l_.insert(idx, 'd') # inserting 'd' into location 2 (starting from 0), ['a', 'b', 'd', 'c']\n",
    "# if idx is lower than 0 it get location 0. If larger than len(l_)-1 it gets value len(l_)-1\n",
    "\n",
    "\n",
    "# popping out last item\n",
    "l_.pop() # yields c, l_ is now ['a', 'b', 'd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inversing key-value relationships (only useful when values are unique)\n",
    "{v:k for k,v in dict_.iteritems()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrappers, Decorators\n",
    "\n",
    "A wrapper/decorator is a useful way to enhance the usage of a function by wrapping a wrapper function around it. [Useful tutorial](http://simeonfranklin.com/blog/2012/jul/1/python-decorators-in-12-steps/)\n",
    "\n",
    "Example:  \n",
    "In this example we will wrap a given function with a time_report function that report the time for that it takes the original function to execute.\n",
    "This is the time reporting function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the function \n",
    "import time\n",
    "\n",
    "def time_report(t0):\n",
    "    tseconds = time.time() - t0\n",
    "    seconds = \"%0.1f\" % tseconds\n",
    "    minutes = \"%0.1f\" % (tseconds / 60.)\n",
    "    hours = \"%0.2f\" % (tseconds / 3600.)\n",
    "    print \"Time s:{}, m:{}, h:{}\".format(seconds, minutes, hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the wrapper\n",
    "\n",
    "def time_report_wrapper(func):\n",
    "    def inner(*args,**kwargs):\n",
    "        t_start = time.time()\n",
    "        #print \"Arguments were: {}, {}\".format(args, kwargs)\n",
    "        result = func(*args,**kwargs)\n",
    "        time_report(t_start)\n",
    "        return result\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two option to wrap `time_report_wrapper` around `your_function`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can either do:\n",
    "def your_function():\n",
    "    {awesome code here}\n",
    "\n",
    "your_function = time_report_wrapper(your_function)\n",
    "# This is done only after you defined your_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Or use the decorator symbol (as of python 2.4)\n",
    "@time_report_wrapper\n",
    "def your_function():\n",
    "    {awesome code here}\n",
    "# I.e, you just \"decorate\" your_function with the wrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping  \n",
    "Adding arguement to a map by creating new function.  \n",
    "E.g assuming a function `mapfunc` that takes an argument `myarg`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "mapfunc = partial(my_function, myarg=myarg)\n",
    "map(mapfunc, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "whatever = deepcopy(whatever_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `numpy` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of bytes\n",
    "np.array([1, 2, 3]).nbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `scipy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sparse matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "sparse_dataset = csr_matrix(dataset) # dataset could be a DataFrame\n",
    "\n",
    "# f sparse (many zeros), two advantages:\n",
    "# (1) substantial reduciton in size\n",
    "# (2) substantial reduction in calculation time (no need for 0*0 calculations)\n",
    "\n",
    "# blog (https://dziganto.github.io/Sparse-Matrices-For-Efficient-Machine-Learning/)\n",
    "# shows speed ups for Naive Bayes, SVM, Logistic Regression. No speed up for Decision Tree based algorithms.\n",
    "# (but will have size reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolating 1D\n",
    "from scipy.interpolate import interp1d\n",
    "fcubic = interp1d(x, y, kind='cubic')\n",
    "\n",
    "y_new = fcubic(x_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit a = 'a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter\n",
    "[jupyter ReadTheDocs](http://jupyter-notebook.readthedocs.io/en/stable/)  \n",
    "[jupyterlab](http://jupyterlab.readthedocs.io/en/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running**   \n",
    "On bash line `jupyter-notebook`  \n",
    "or if you have a port number in mind (e.g 9039):   \n",
    "`jupyter-notebook --port=9039`     \n",
    "\n",
    "**Extensions**  \n",
    "See [`jupyter_contrib_nbextensions`](https://github.com/ipython-contrib/jupyter_contrib_nbextensions)  \n",
    "\n",
    "**Embedding Image**    \n",
    "In Markdown mode:  \n",
    "`![title](./example_graph.png)`    \n",
    "\n",
    "[Tutorial with advanced tips/tricks](https://blog.dominodatalab.com/lesser-known-ways-of-using-notebooks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| This | is   | how  |  to  |\n",
    "|------|------|------|------|\n",
    "| make |  a   |table |  :-) |\n",
    "| 1    |  2   |  3   |   4  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| This | is   | how  |  to  |\n",
    "|------|------|------|------|\n",
    "| make |  a   |table |  :-) |\n",
    "| 1    |  2   |  3   |   4  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas  \n",
    "[Tutorial: pandas in 10 minutes](http://pandas.pydata.org/pandas-docs/stable/10min.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading\n",
    "xlsx = pd.ExcelFile(\"file.xlsx\")\n",
    "\n",
    "print xlsx.sheet_names\n",
    "df = xlsx.parse(xlsx.sheet_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# writing\n",
    "writer = pd.ExcelWriter(\"example_file.xls\")\n",
    "df1.to_excel(writer, sheet_name=\"1st result\")\n",
    "df2.to_excel(writer, sheet_name=\"2nd result\")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniqueness = np.array([len(df[col].unique()) for col in df.columns])\n",
    "completeness = df.notnull().sum()\n",
    "norm = 100./ df.shape[0]\n",
    "\n",
    "df_meta = pd.DataFrame({'complete': completeness, \n",
    "                        'complete_%': completeness * norm,\n",
    "                        'unique': uniqueness, \n",
    "                        'unique_%': uniqueness * norm\n",
    "                       }).loc[df.columns][['complete', 'unique','complete_%', 'unique_%']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `pandas_profiling`\n",
    "[source Github](https://github.com/pandas-profiling/pandas-profiling/blob/master/examples/meteorites.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas_profiling  # pip install pandas-profiling\n",
    "pandas_profiling.ProfileReport(df)\n",
    "\n",
    "pfr = pandas_profiling.ProfileReport(df)\n",
    "pfr.to_file(\"./example.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_order = [df.index.name] + df.columns.tolist() # assuming the index.name is not null\n",
    "df_meta = pfr.description_set['variables'].loc[l_order] # is a DF with MetaData\n",
    "\n",
    "print df_meta.shape\n",
    "df_meta.head(4)\n",
    "\n",
    "#E.g, pfr.description_set['variables']['type'] is the columns type:\n",
    "#Numeric\n",
    "#Categorical\n",
    "#Boolean\n",
    "#Date\n",
    "#Text (Unique)\n",
    "#Rejected\n",
    "#Unsupported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# useful to determine ordinal features examining the number of distinct \n",
    "l_numerical = df_meta[df_meta['type'] == 'NUM'].index.tolist()\n",
    "print df_meta[df_meta['type'] == 'NUM']['distinct_count'].sort_values()\n",
    "\n",
    "\n",
    "# useful to determine binary/multicategorical features examining the number of distinct \n",
    "sr_distCounts =  df_meta[df_meta['type'] == 'CAT']['distinct_count'].sort_values()\n",
    "print sr_distCounts.head(6)\n",
    "\n",
    "\n",
    "def meta_to_types(meta, data):\n",
    "    # distinguishes between the types: unary, binary, mutlicategorical\n",
    "    sr_distCounts =  meta[meta['type'] == 'CAT']['distinct_count'].sort_values()\n",
    "\n",
    "    l_unary = []\n",
    "    l_binary = []\n",
    "    for col, distinct in sr_distCounts.iteritems():\n",
    "        # because the profiler considers NaN as a distinct entry this code is more cautious \n",
    "        if distinct <= 3: # extereme case 3 values including NaN\n",
    "            distinct_notNan = len(data[col].value_counts(dropna=True))\n",
    "            if distinct_notNan == 1: # Unary\n",
    "                l_unary.append(col)\n",
    "                print col, distinct_notNan, \"Unary\"\n",
    "            elif distinct_notNan == 2: # Binary\n",
    "                l_binary.append(col)\n",
    "                print col, distinct_notNan,\"Binary\"\n",
    "\n",
    "    l_multiCategorical = list( (set(sr_distCounts.index.tolist()) -  set(l_binary) ) - set(l_unary)  )   \n",
    "\n",
    "    return l_unary, l_binary, l_multiCategorical\n",
    "\n",
    "l_unary, l_binary, l_multiCategorical = meta_to_types(df_meta, df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ordinals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----- \n",
    "\n",
    "# Does not currently support type = \n",
    "#Boolean -- need to integrate with 'binary'\n",
    "#Date\n",
    "#Text (Unique)\n",
    "#Rejected\n",
    "#Unsupported\n",
    "\n",
    "# once looked at and understood, convert to ordinal\n",
    "l_ordinal = ['OverallCond', 'OverallQual']\n",
    "\n",
    "print len(l_binary), len(l_multiCategorical), len(l_numerical)\n",
    "\n",
    "l_binary = list( set(l_binary) - set(l_ordinal))\n",
    "l_multiCategorical = list( set(l_multiCategorical) - set(l_ordinal))\n",
    "l_numerical =  list( set(l_numerical) - set(l_ordinal))\n",
    "\n",
    "print len(l_binary), len(l_multiCategorical), len(l_numerical), len(l_ordinal)\n",
    "\n",
    "\n",
    "# ------ creating new df_metaF (as in final) ---------\n",
    "df_metaF = df_meta.copy()\n",
    "\n",
    "df_metaF.loc[l_unary, 'data_type'] = 'unary'\n",
    "df_metaF.loc[l_binary, 'data_type'] = 'binary'\n",
    "df_metaF.loc[l_ordinal, 'data_type'] = 'ordinal'\n",
    "df_metaF.loc[l_multiCategorical, 'data_type'] = 'multiCategorical'\n",
    "df_metaF.loc[l_numerical, 'data_type'] = 'numerical'\n",
    "\n",
    "print df_metaF.shape\n",
    "print df_metaF['type'].value_counts()\n",
    "print '-' * 20\n",
    "print df_metaF['data_type'].value_counts()\n",
    "\n",
    "df_metaF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing on more complete entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# counts of special values overall\n",
    "print df_meta[['n_infinite', 'n_missing', 'n_zeros']].sum()\n",
    "\n",
    "# percentages of potentially missing data per column\n",
    "def print_incompleteness(meta, size, col_sort = 'n_zeros', ascending=False, normalise=True):\n",
    "    df_ = meta[['n_infinite', 'n_missing', 'n_zeros']]\n",
    "    if normalise:\n",
    "        df_ = df_ * 100. / size\n",
    "    print (df_ ).sort_values(col_sort, ascending=ascending)\n",
    "    \n",
    "print_incompleteness(df_meta, df.shape[0], normalise=False, col_sort='n_missing') # 'n_zeros' 'n_missing' 'n_infinite'\n",
    "\n",
    "# lists of column that have many missing data (consider not using in first iteration)\n",
    "# ALSO consider creating a binary feature of each (as in True: has value, False: Missing)\n",
    "missing_thresh = 0.15\n",
    "l_missingHigh = df_meta[df_meta['n_missing'] > missing_thresh].index.tolist()\n",
    "\n",
    "zeros_thresh = 0.15\n",
    "l_zerosHigh = df_meta[df_meta['n_zeros'] > zeros_thresh].index.tolist()\n",
    "\n",
    "\n",
    "# ---------- Focusing on a Subset of Features ------------- \n",
    "l_explore = list( set(df_train.columns.tolist()) - set(l_missingHigh) - set(l_zerosHigh))\n",
    "\n",
    "print df_train.shape\n",
    "df_explore = df_train[l_explore]\n",
    "print df_explore.shape\n",
    "\n",
    "print df_metaF.loc[df_explore.columns, 'data_type'].value_counts()\n",
    "print_incompleteness(df_metaF.loc[df_explore.columns], df_train.shape[0], normalise=False, col_sort='n_missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing   \n",
    "[A Comparison of Six Methods for Missing Data Imputation, Schmitt, Mandel, Guedj](https://www.omicsonline.org/open-access/a-comparison-of-six-methods-for-missing-data-imputation-2155-6180-1000224.pdf)\n",
    "#### probabilistic method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_nonunary =  list( set(df_explore.tolist()) -  set(df_metaF.loc[df_explore.columns][df_metaF.loc[df_explore.columns, 'data_type'] == 'unary'].index) )\n",
    "\n",
    "imputing_method = 'probabilstic' # should cover all types \n",
    "l_Zerospecial = [] #special treaement - NaNs get zero\n",
    "\n",
    "# other methods: mean, median\n",
    "for col in l_nonunary:\n",
    "    bool_ = (df_explore[col].isnull()  )  # |  df_explore[col].map(np.isinf) \n",
    "    idx = df_explore[bool_].index\n",
    "    idx_use = df_explore[~bool_].index\n",
    "    \n",
    "    if col in l_Zerospecial:\n",
    "        df_explore.loc[idx, col] = 0\n",
    "    else:\n",
    "        df_explore.loc[idx, col] = pd.Series(df_explore.loc[idx_use, col].sample(n=len(idx), replace=True).tolist(), index=idx)\n",
    "    \n",
    "    print col, len(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fuzzy K-means    \n",
    "[sklearn-extensions](http://wdm0006.github.io/sklearn-extensions/fuzzy_k_means.html)  \n",
    "[scikit-fuzzy](http://pythonhosted.org/scikit-fuzzy/auto_examples/plot_cmeans.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_binary_explore = [col  for col in l_binary if col in df_explore.columns.tolist()]\n",
    "l_multiCategorical_explore = [col  for col in l_multiCategorical if col in df_explore.columns.tolist()]\n",
    "\n",
    "l_categorical_explore = l_binary_explore + l_multiCategorical_explore\n",
    "\n",
    "df_dummies = df_explore[[]]\n",
    "for col in l_categorical_explore:\n",
    "    \n",
    "    df_dummies_temp = pd.get_dummies(df_explore[col])\n",
    "    df_dummies_temp.columns = df_dummies_temp.columns.map(lambda x: \"{}_{}\".format(col, x))\n",
    "    \n",
    "    if col in l_binary:\n",
    "        # dropping second column\n",
    "        l_cols_dummy = sorted(df_dummies_temp.columns) #sorted(df_training[col].unique())\n",
    "        df_dummies_temp.drop(l_cols_dummy[-1], axis=1, inplace=True)\n",
    "    \n",
    "    df_dummies = df_dummies.join(df_dummies_temp)\n",
    "    \n",
    "    print col, df_dummies.shape\n",
    "  \n",
    "print df_dummies.shape\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Describing the standard deviation of each dummy variable, and selecting the important ones\n",
    "sr_std_dummies = df_dummies.describe().loc['std'].sort_values()\n",
    "\n",
    "print sr_std_dummies.describe()\n",
    "\n",
    "# ===========\n",
    "std_thresh = 0.2\n",
    "\n",
    "l_dummies_use = sr_std_dummies[sr_std_dummies >= std_thresh].index\n",
    "for col in sorted(l_dummies_use):\n",
    "    print col\n",
    "print df_dummies[l_dummies_use].shape\n",
    "df_dummies[l_dummies_use].head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print df_explore.shape\n",
    "df_explore = df_explore.drop(l_categorical_explore, axis=1).join(df_dummies[l_dummies_use])\n",
    "\n",
    "print df_explore.shape\n",
    "df_explore.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select numerical column columns \n",
    "\"\"\"\n",
    "bool_ = df_meta['type'] == 'NUM'\n",
    "\n",
    "# might want a minimu value threshold\n",
    "numerical_thresh = 10\n",
    "bool_ &= df_meta['distinct_count'] > numerical_thresh \n",
    "\n",
    "l_col_manyNum = df_meta[bool_].index.tolist()\n",
    "\n",
    "# might want to exclude column with many missing data\n",
    "l_col_manyNum = list(set(l_col_manyNum) - set(l_missingHigh) - set(l_zerosHigh))\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using visual aid. l_col_manyNum is defined above\n",
    "l_numerical_explore = [col  for col in l_numerical if col in df_explore.columns.tolist()]\n",
    "\n",
    "npanels = len(l_numerical_explore)\n",
    "\n",
    "ncols = 4\n",
    "nrows = npanels / ncols + np.sum( (npanels % ncols) != 0 )\n",
    "\n",
    "width, height = 3, 4 \n",
    "plt.figure(figsize=(width*ncols, height*nrows))\n",
    "\n",
    "for panel, col in enumerate(l_numerical_explore):\n",
    "    plt.subplot(nrows, ncols, panel + 1)\n",
    "    values = df[col][df[col].notnull()]\n",
    "    \n",
    "    plt.hist(values)\n",
    "    plt.title(col, fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the columns that should be logarithmic\n",
    "l_toLog = ['LotArea', 'SalePrice']\n",
    "print df_explore[l_toLog].describe(percentiles=np.arange(0., 1.1, 0.1)).T[['10%', '90%']]\n",
    "\n",
    "\n",
    "# creating logarithmic DF\n",
    "df_logs = df_train[[]]\n",
    "for col in l_toLog:\n",
    "    print col\n",
    "    \n",
    "    df_logs = df_logs.join(df[col].map(np.log10))\n",
    "  \n",
    "df_logs.columns = df_logs.columns.map(lambda x: \"{}_log10\".format(x))\n",
    "print df_logs.shape\n",
    "df_logs.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# joining with previous data, and updating the list of columns (df_meta not updated!)\n",
    "df = df.drop(l_toLog, axis=1).join(df_logs)\n",
    "\n",
    "l_numerical_explore = list(set(l_numerical_explore) - set(l_toLog)) + df_logs.columns.tolist()\n",
    "\n",
    "# now use the visualisation from before again to see if that made sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_standard = df.copy()\n",
    "\n",
    "l_numerical_standard = []\n",
    "\n",
    "for col in l_numerical_explore:\n",
    "    mu = df_standard[col].mean()\n",
    "    sigma = df_standard[col].std()\n",
    "    \n",
    "    \n",
    "    col_standard = \"{}_standard\".format(col)\n",
    "    df_standard[col_standard] = (df_standard[col] - mu) / sigma\n",
    "    df_standard.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    l_numerical_standard.append(col_standard)\n",
    "    \n",
    "l_numerical_explore = list(l_numerical_standard)\n",
    "print df_standard[l_numerical_explore].shape\n",
    "df_standard[l_numerical_explore].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# correlations\n",
    "\n",
    "\n",
    "col_target = 'SalePrice_log10_standard'\n",
    "\n",
    "df_corrPearson = df_standard[l_numerical_explore].corr(method='pearson')[[col_target]]\n",
    "df_corrSpearman = df_standard[l_numerical_explore].corr(method='spearman')[[col_target]]\n",
    "\n",
    "df_corr_target = 100. * (df_corrPearson).join(df_corrSpearman, lsuffix='_pearson', rsuffix='_spearman')\n",
    "df_corr_target.drop(col_target, axis=0, inplace=True)\n",
    "df_corr_target['spearman_minus_pearson'] = df_corr_target[df_corr_target.columns[1]] - df_corr_target[df_corr_target.columns[0]]\n",
    "df_corr_target.sort_values('spearman_minus_pearson', ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# all numerical vs. all numerical\n",
    "df_corrPearson = pfr.description_set['correlations']['pearson']\n",
    "df_corrSpearman = pfr.description_set['correlations']['spearman']\n",
    "\n",
    "# all numerical vs. target numerical\n",
    "# spearman is less sensitive than pearson to outliers, and hence will show strong correlations and anticorrelations\n",
    "# hence the difference between them should indicate that the feature has outliers.\n",
    "col_target = 'SalePrice'\n",
    "\n",
    "df_corr_target = 100. * (df_corrPearson.loc[[col_target]].T).join(df_corrSpearman.loc[[col_target]].T, lsuffix='_pearson', rsuffix='_spearman')\n",
    "df_corr_target.drop(col_target, axis=0, inplace=True)\n",
    "df_corr_target['spearman_minus_pearson'] = df_corr_target[df_corr_target.columns[1]] - df_corr_target[df_corr_target.columns[0]]\n",
    "df_corr_target.sort_values('spearman_minus_pearson', ascending=False, inplace=True)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date-Time Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(df['DOB'], format=\"%Y/%m/%d\", errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dealing with hours\n",
    "pd.to_datetime(\"1:30 AM\", format=\"%I:%M %p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Categorical Data (Nominal Data)](https://en.wikipedia.org/wiki/Level_of_measurement#Nominal_level)\n",
    "\n",
    "> The nominal type differentiates between items or subjects based only on their names or (meta-)categories and other qualitative classifications they belong to; thus dichotomous data involves the construction of classifications as well as the classification of items. Discovery of an exception to a classification can be viewed as progress. Numbers may be used to represent the variables but the numbers do not have numerical value or relationship  \n",
    "\n",
    "E.g, gender, nationality, ethnicity, language, genre, style, biological species, and form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# binning numerical to categorical. E.g, Age to Age_brackets\n",
    "\n",
    "dict_dict_min, dict_dict_max = OrderedDict(), OrderedDict()\n",
    "\n",
    "col = 'Age'\n",
    "dict_dict_min[col], dict_dict_max[col] = OrderedDict(), OrderedDict()\n",
    "dict_dict_min[col]['18-34'] = 0. \n",
    "dict_dict_min[col]['35-49'] = 35.\n",
    "dict_dict_min[col]['50-64'] = 50.\n",
    "\n",
    "\"\"\"\n",
    "# or more generically \n",
    "dict_dict_min[col]['18-24'] = 0. # special - 18-24\n",
    "for minval in np.arange(25, 65, 5): # generic - leaps of 5 years\n",
    "    key = \"{}-{}\".format(str(minval), str(minval + 4))\n",
    "    dict_dict_min[col][key] = float(minval)\n",
    "dict_dict_min[col]['65+'] = 65. # special - 65 and over\n",
    "\"\"\"\n",
    "\n",
    "dict_dict_min[col]['65+'] = 65.\n",
    "\n",
    "keys = dict_dict_min[col].keys()\n",
    "for ikey, key in enumerate(keys[:-1]):\n",
    "    dict_dict_max[col][key] = dict_dict_min[col][keys[ikey + 1]]\n",
    "dict_dict_max[col][keys[-1]] = 20000.\n",
    "\n",
    "def numeric2brackets(df, col, col_bracket=None):\n",
    "    if not col_bracket:\n",
    "        col_bracket = \"{}_bracket\".format(col)\n",
    "    \n",
    "    dict_min = dict_dict_min[col]\n",
    "    dict_max = dict_dict_max[col]\n",
    "    \n",
    "    for key in dict_min.keys():\n",
    "        print key, dict_min[key], dict_max[key]\n",
    "        indexes_temp = df[(df[col] >= dict_min[key]) & (df[col] < dict_max[key])].index\n",
    "        df.loc[indexes_temp, col_bracket] = key\n",
    "    print \"-----\"\n",
    "    print df[col_bracket].value_counts(dropna=False, normalize=True) \n",
    "    \n",
    "numeric2brackets(df_, 'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Ordinal Data](https://en.wikipedia.org/wiki/Ordinal_data) \n",
    "> Ordinal data is a categorical, statistical data type where the variables have natural, ***ordered categories and the distances between the categories is not known***. The ordinal scale is distinguished from the nominal scale by having ordered categories. It also differs from interval and ratio scales by not having category widths that represent equal increments of the underlying attribute.\n",
    "\n",
    "Examples:  \n",
    "\n",
    "Likert scale:   \n",
    "Like=1\tLike Somewhat=2\tNeutral=3\tDislike Somewhat=4   Dislike=5  \n",
    "\n",
    "Income groupings \\$0-\\$19,999, \\$20,000-\\$39,999, \\$40,000-\\$59,999, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Categorical to Ordinal\n",
    "l_order = dict_dict_min['Age'].keys() # e.g ['18-34', '35-54', '55-64', '65+']\n",
    "df_.loc[:, 'Age_bracket'] = pd.Categorical(df_['Age_bracket'], categories=l_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grouping and yielding by size\n",
    "l_cols = ['Age_bracket', 'Gender']\n",
    "df_.groupby(l_cols).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grouping and yielding by percentage within group.\n",
    "l_cols = ['Age_bracket', 'Gender']\n",
    "df_.groupby(l_cols).size().groupby(level=0).apply(lambda x: x * 100./ x.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highlighting `DataFrame`\n",
    "[pandas docs](http://pandas.pydata.org/pandas-docs/stable/style.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# highlighting a null cell\n",
    "df.style.highlight_null(null_color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "\n",
    "# highlighting by gradient\n",
    "df.style.background_gradient(cmap=cm)\n",
    "\n",
    "# highlighting by gradient only on a subset of columns \n",
    "# and highlighting 0 values\n",
    "# see (highlight_0 below)\n",
    "df.style.background_gradient(cmap=cm, subset=['net_total']).applymap(highlight_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# highlight particular cell\n",
    "\n",
    "def highlight_0(val, color_highlight='red'):\n",
    "    result = 'background-color: {}'.format(color_highlight) if val == 0 else ''\n",
    "    return result \n",
    "\n",
    "def color_0_red(val):\n",
    "    \"\"\"\n",
    "    Takes a scalar and returns a string with\n",
    "    the css property `'color: red'` for negative\n",
    "    strings, black otherwise.\n",
    "    \"\"\"\n",
    "    color = 'red' if val == 0 else 'black'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "df.style.applymap(highlight_0) # color_0_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# highlight max in each row\n",
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "df.style.apply(highlight_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "[My machine learning notes](http://bit.ly/2DCfdzP)  \n",
    "If you care to comment there, please be kind as it is a work in progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "[scikit-learn module](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "clf_linear = LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=-1)\n",
    "clf_linear.fit(df_X.values, df_y['true'].values)\n",
    "\n",
    "# Coefficients\n",
    "print pd.Series(clf_linear.coef_, index=df_X.columns)\n",
    "\n",
    "# score (coefficient of determination)\n",
    "# R^2 is defined as (1 - u/v)\n",
    "# u = ((y_true - y_pred) ** 2).sum()\n",
    "# v = ((y_true - y_true.mean()) ** 2).sum()\n",
    "clf_linear.score(df_XCV.values, df_yCV['true'].values)\n",
    "\n",
    "# predictions\n",
    "df_y['prediction'] = pd.Series(clf_linear.predict(df_X), index=df_y.index)\n",
    "\n",
    "# plotting \n",
    "plt.scatter(df_y['true'], df_y['prediction'], s=5)\n",
    "\n",
    "min_, max_ = df_y['true'].min(), df_y['true'].max()\n",
    "plt.plot([min_, max_], [min_, max_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y = pd.DataFrame(clf_rf.predict_proba(df_X_validate), columns=[0,'prediction'], index=df_X_validate.index)[['prediction']]\n",
    "df_Y = df_Y.join(df_y_validate[[col_target]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Histogram probabilities by target variable\n",
    "\n",
    "idx_1sTest = df_Y[df_Y[col_target] == 1].index\n",
    "idx_0sTest = df_Y[df_Y[col_target] == 0].index\n",
    "\n",
    "fontsize = 15\n",
    "dbins = 0.2\n",
    "bins = np.arange(0., 1.0 + dbins, dbins)\n",
    "normed = True\n",
    "plt.hist(df_Y.loc[idx_1sTest, 'prediction'], bins=bins, normed=normed, label='fitness=1')\n",
    "plt.hist(df_Y.loc[idx_0sTest, 'prediction'], bins=bins, normed=normed, alpha=0.7,label='fitness=0')\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.xlabel('probability fitness=1', fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC and Precision Recall Curves\n",
    "[scikit-learn precision-recall curve plotting](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ROC curve, Precision-Recall curve\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "# y_true: 0 or 1\n",
    "# y_score: score (fraction) between 0 and 1, including (e.g, probability)\n",
    "\n",
    "# ROC metrics\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_true, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Precision Recall metrics\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_true, y_score)\n",
    "average_precision = average_precision_score(y_true, y_score) \n",
    "# Note: average_precision_score implementation is restricted to the binary classification task or multilabel classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T19:57:24.713615Z",
     "start_time": "2018-02-20T19:57:24.474327"
    }
   },
   "source": [
    "where `average_precision_score` is \n",
    "![](http://scikit-learn.org/stable/_images/math/4c2834ed52d8a363dc694e02ad124e8c86070706.png) \n",
    "where $P_n$ and $R_n$ are the precision and recall at the nth threshold. This implementation is not interpolated and is different from computing the area under the precision-recall curve with the trapezoidal rule, which uses linear interpolation and can be too optimistic.   \n",
    "Note: this implementation is restricted to the binary classification task or multilabel classification task. [source: scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting \n",
    "plt.figure(figsize=(16,5))\n",
    "\n",
    "lw = 2\n",
    "fontsize=16\n",
    "\n",
    "# ========= ROC curve =========\n",
    "plt.subplot(1, 2, 1)\n",
    "label = 'AUC %0.2f' % roc_auc\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label=label)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1-Specificity)', fontsize=fontsize)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=fontsize)\n",
    "plt.title('Receiver Operating Characteristic', fontsize=fontsize)\n",
    "plt.legend(loc=\"lower right\", fontsize=fontsize)\n",
    "\n",
    "# ========= Precision Recall curve =========\n",
    "plt.subplot(1, 2, 2)\n",
    "label = 'Average Precision = %0.2f' % average_precision\n",
    "plt.plot(recall, precision, color='darkorange', lw=lw, label=label)\n",
    "yval = precision[0]\n",
    "plt.plot([0, 1], [yval, yval], color='navy', lw=lw, linestyle='--')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall/Sensitivity/(True Positive Rate)', fontsize=fontsize)\n",
    "plt.ylabel('Precision', fontsize=fontsize)\n",
    "plt.title('Precision Recall Curve', fontsize=fontsize)\n",
    "plt.legend(loc=\"lower right\", fontsize=fontsize)\n",
    "\n",
    "# --- f1 score contours ---\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "f_scores = np.append(f_scores, 0.9)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    \n",
    "# ---- thresholds -----\n",
    "from scipy.interpolate import interp1d\n",
    "fcubic_precision = interp1d(thresholds_pr, precision[:-1], kind='cubic')\n",
    "fcubic_recall = interp1d(thresholds_pr, recall[:-1], kind='cubic')\n",
    "\n",
    "thresholds_ = np.arange(0.3, 1, 0.1)\n",
    "bool_ = (np.array(thresholds_) >= min(thresholds_pr)) & (np.array(thresholds_) <= max(thresholds_pr))\n",
    "thresholds_plot = np.array(thresholds_)[np.where(bool_)]\n",
    "\n",
    "precision_plot = fcubic_precision(thresholds_plot)\n",
    "recall_plot = fcubic_recall(thresholds_plot)\n",
    "plt.scatter( recall_plot, precision_plot, marker='o', color='red')\n",
    "\n",
    "dx, dy = 0., 0.02\n",
    "for i, threshold in enumerate(thresholds_plot):\n",
    "    print plt.annotate(xy=(recall_plot[i] + dx, precision_plot[i] + dy), s=\"{}\".format(threshold), color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# determine threshold by observation\n",
    "factor = 1 # relative cost of error on recall (1-recall) compared to cost of error of precision (1-precision)\n",
    "\n",
    "plt.plot(thresholds_pr, factor * (1 - recall[:-1]), label='${}\\cdot$(1 - Recall)'.format(factor))\n",
    "plt.plot(thresholds_pr, 1 - precision[:-1], label='(1 - Precision)')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# determin the class\n",
    "\n",
    "thresh = 0.6\n",
    "\n",
    "bool_ = df_Y['prediction'] >= thresh\n",
    "idx_1s_predict = df_Y[bool_].index\n",
    "idx_0s_predict = df_Y[~bool_].index\n",
    "\n",
    "df_Y.loc[idx_1s_predict, 'predict_class'] = 1\n",
    "df_Y.loc[idx_0s_predict, 'predict_class'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision_recall_fscore_support(df_Y[col_target], df_Y['predict_class'], beta=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix (wight DF highlighting)\n",
    "df_confusion = pd.DataFrame(confusion_matrix(df_Y[col_target], df_Y['predict_class']), index=['0_true', '1_true'], columns=['0_predict', '1_predict'])\n",
    "df_confusion.style.background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Precision (by column -- predicted values are base)\n",
    "(df_confusion * 100./ df_confusion.sum(axis=0)).style.background_gradient(cmap=cm, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recall (by row -- true values are base; notice the Transposes)\n",
    "(df_confusion.T * 100./ df_confusion.T.sum(axis=0)).T.style.background_gradient(cmap=cm, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "[scikit-learn](http://scikit-learn.org/stable/modules/naive_bayes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bernoulli Naive Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desicion Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining a Tree\n",
    "[with scikit-learn](http://scikit-learn.org/stable/modules/tree.html#tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given a \n",
    "#clf = tree.DecisionTreeClassifier, \n",
    "#      tree.DecisionTreeRegressor,\n",
    "#      tree.ExtraTreeClassifier,\n",
    "#      tree.ExtraTreeRegressor\n",
    "from sklearn import tree\n",
    "import graphviz # pip install graphviz\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"iris\") # produces iris.pdf\n",
    "\n",
    "# note that to extract from a tree ensemble clf_ensemble (e.g, RandomForest) do: \n",
    "# clf = clf_ensemble.estimators_[i_clf]\n",
    "# where i_clf is the Tree estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# more options (labeling, coloring)\n",
    "class_names = ['0', '1'] # example\n",
    "feature_names = df_X.columns.tolist()\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                     feature_names=feature_names,  \n",
    "                     class_names=class_names,  \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph.render(\"tree_test\") # produces tree_test.pdf\n",
    "graph # will show the tree in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Ensembles\n",
    "`sklearn.ensemble` [scikit-learn modules](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)  \n",
    "\n",
    "ensemble.AdaBoostClassifier([…])\tAn AdaBoost classifier.   \n",
    "ensemble.AdaBoostRegressor([base_estimator, …])\tAn AdaBoost regressor.    \n",
    "ensemble.BaggingClassifier([base_estimator, …])\tA Bagging classifier.    \n",
    "ensemble.BaggingRegressor([base_estimator, …])\tA Bagging regressor.   \n",
    "ensemble.ExtraTreesClassifier([…])\tAn extra-trees classifier.   \n",
    "ensemble.ExtraTreesRegressor([n_estimators, …])\tAn extra-trees regressor.   \n",
    "ensemble.GradientBoostingClassifier([loss, …])\tGradient Boosting for classification.   \n",
    "ensemble.GradientBoostingRegressor([loss, …])\tGradient Boosting for regression.  \n",
    "ensemble.IsolationForest([n_estimators, …])\tIsolation Forest Algorithm   \n",
    "ensemble.RandomForestClassifier([…])\tA random forest classifier.   \n",
    "ensemble.RandomForestRegressor([…])\tA random forest regressor.    \n",
    "ensemble.RandomTreesEmbedding([…])\tAn ensemble of totally random trees.   \n",
    "ensemble.VotingClassifier(estimators[, …])\tSoft Voting/Majority Rule classifier for unfitted estimators.  \n",
    "\n",
    "Partial dependence plots for tree ensembles.\n",
    "\n",
    "ensemble.partial_dependence.partial_dependence(…)\tPartial dependence of target_variables.   \n",
    "ensemble.partial_dependence.plot_partial_dependence(…)\tPartial dependence plots for features   \n",
    "\n",
    "**Metrics**\n",
    "\n",
    "Classification  \n",
    "Gini function: $1 - (w^2 + (1-w)^2)$, \n",
    "where $w$ is probablity for being 1 (or 0; note the $1-w$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "clf_rf = RandomForestRegressor()\n",
    "clf_rf.fit(df_X, df_y['true'])\n",
    "\n",
    "# important features\n",
    "sr_important = pd.Series(clf_rf.feature_importances_, index=df_X.columns).sort_values(ascending=True)\n",
    "sr_important.tail(10).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction  \n",
    "\n",
    "Linear and Non-Linear  \n",
    "\n",
    "\n",
    "> Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications. [source](http://scikit-learn.org/stable/modules/manifold.html#manifold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis   \n",
    "**PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_standard - assuming all features are numerical and standardised\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# exploring the variance expected to maintain\n",
    "n_components = 21 # number of compenents depending on the amount of variance to preserve\n",
    "\n",
    "print df_standard.shape\n",
    "random_state = 1\n",
    "pca = PCA(n_components=n_components, random_state=random_state)\n",
    "pca.fit(df_standard.values)\n",
    "\n",
    "print pca.explained_variance_ratio_ * 100., np.sum(pca.explained_variance_ratio_) * 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training set in terms of PCA eigen vectors\n",
    "print df_standard.shape\n",
    "df_training_pca = pd.DataFrame(pca.transform(df_standard.values), index=df_standard.index)\n",
    "df_training_pca.columns = df_training_pca.columns.map(lambda x: \"PCA_{}\".format(x))\n",
    "\n",
    "print df_training_pca.shape\n",
    "\n",
    "df_training_pca.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Examining the components themselves \n",
    "df_components = pd.DataFrame(pca.components_, index=df_training_pca.columns, columns=df_standard.columns)\n",
    "\n",
    "print df_components.shape\n",
    "df_components\n",
    "\n",
    "# ============= plotting all components (sorted by importance of one)\n",
    "# notice that here I use np.abs\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "sort_by = 'PCA_0'\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "ax = sns.heatmap(df_components.T.apply(np.abs).sort_values(sort_by) , annot=False, fmt=\"0.1f\", cmap='viridis')\n",
    "\n",
    "# ============ Examining one component\n",
    "pca_component = 'PCA_0'\n",
    "sr_component = df_components.loc[pca_component].map(np.abs).sort_values(ascending=False)\n",
    "sr_plot = df_components.loc[pca_component].loc[sr_component[sr_component > 0.1].index]\n",
    "sr_plot\n",
    "\n",
    "# ============ Scatter plot of entries \n",
    "plt.scatter(df_training_pca['PCA_0'], df_training_pca['PCA_1'], s=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Multiple Correspondence Analysis\n",
    "**MSA**  \n",
    "[python source](https://pypi.python.org/pypi/mca/1.0.2)  \n",
    "[python tutorial](http://nbviewer.jupyter.org/github/esafak/mca/blob/master/docs/mca-BurgundiesExample.ipynb) ([original paper](https://www.utdallas.edu/~herve/Abdi-MCA2007-pretty.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-distributed Stochastic Neighbor Embedding \n",
    "**t-SNE**    \n",
    "[scikit-learn module](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)      \n",
    "[Tutorial on Manifold Learning ](http://scikit-learn.org/stable/modules/manifold.html#t-sne)   \n",
    "[blog about high dimensional datasets](https://medium.com/@luckylwk/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b)  \n",
    "[t-SNE tutorial](https://github.com/oreillymedia/t-SNE-tutorial) \n",
    "\n",
    "[Baloo's Song (Bare necessities)](https://www.youtube.com/watch?v=9ogQ0uge06o)  \n",
    "\n",
    "> The disadvantages to using t-SNE are roughly:\n",
    "* t-SNE is computationally expensive, and can take several hours on million-sample datasets where PCA will finish in seconds or minutes\n",
    "* The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.\n",
    "* The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error.\n",
    "* Global structure is not explicitly preserved. This is problem is mitigated by initializing points with PCA (using init=’pca’). [source: scikit-learn](http://scikit-learn.org/stable/modules/manifold.html#manifold)  \n",
    "\n",
    "[Tutorial: How to Use t-SNE Effectively (distill.pub)](https://distill.pub/2016/misread-tsne/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder\n",
    "> An autoencoder, autoassociator or Diabolo network is an artificial neural network used for unsupervised learning of efficient codings.†au The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. [(Wikipedia)](https://en.wikipedia.org/wiki/Autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "[scikit-learn API](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means\n",
    "[scikit-learn module](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)  \n",
    "[scikit-learn user's guide](http://scikit-learn.org/stable/modules/clustering.html#k-means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assumes df_training_pca is all numerical (and possibly PCA after standardising)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "max_iter = 500\n",
    "\n",
    "n_clusters = 4 # change according to Knee bend\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, max_iter=max_iter).fit(df_training_pca.values)\n",
    "print kmeans.n_iter_, \" Iterations in practice\"\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Examining results \n",
    "colT = 'segment'\n",
    "\n",
    "df_results = df_training_pca.copy()\n",
    "df_results[colT] = pd.Series(kmeans.labels_, index=df_standard.index)\n",
    "\n",
    "print df_results.shape\n",
    "df_results.head(4)\n",
    "\n",
    "# ======= plotting =======\n",
    "l_cols_plot = ['PCA_0', 'PCA_1', 'PCA_2', 'PCA_3']\n",
    "\n",
    "\n",
    "df_plot = df_results[l_cols_plot + [colT]] #.sample(10000)\n",
    "\n",
    "g = sns.pairplot(df_plot, hue=colT, vars=l_cols_plot, size=5., diag_kws={\"alpha\": 0.7, 'histtype':'step', 'linewidth':4.}, plot_kws={\"alpha\": 0.4, \"s\":10}) #, plot_kws={\"alpha\": '0.7'})\n",
    "for i, j in zip(*np.triu_indices_from(g.axes, 1)):\n",
    "    g.axes[i, j].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tagging the segments by segment number\n",
    "df_segment = pd.DataFrame(pd.Series(kmeans.labels_, index=df_standard.index), columns=['segment_number'])\n",
    "print df_segment['segment_number'].value_counts(normalize=True)\n",
    "print df_segment.shape\n",
    "df_segment.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Knee curve\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "X = df_training_pca.values.copy()\n",
    "\n",
    "distortions = []\n",
    "K = range(1,20)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k, max_iter=max_iter).fit(X)\n",
    "    print k, kmeanModel.n_iter_\n",
    "    kmeanModel.fit(X)\n",
    "    # for each point - \n",
    "    # (1) calculate the distance to the nearest cluster: np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)\n",
    "    # for all points together\n",
    "    # (2) calculate average distance of each point sum(PREVIOUS) / N_points\n",
    "    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'b-o')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier/Novelty Detection\n",
    "\n",
    "[Andrew Ng](https://www.youtube.com/watch?v=ZKaOfJIjMRg) recommends manipulating all features to make them look gaussian by using Log, sqrt (or other powers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Covariance Estimation \n",
    "[scikit-learn tutorial](http://scikit-learn.org/stable/auto_examples/covariance/plot_mahalanobis_distances.html#sphx-glr-auto-examples-covariance-plot-mahalanobis-distances-py)\n",
    "\n",
    "> **Mahalanobis Distance** is a multi-dimensional generalization of the idea of measuring how many standard deviations away a point P is from the mean of distribution D.   \n",
    ">If each of these axes is rescaled to have unit variance, then Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set.\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Mahalanobis_distance)\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/022088abeaaecdb767fb86a1b65e28ec566a1c36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.covariance import EmpiricalCovariance, MinCovDet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit a Minimum Covariance Determinant (MCD) robust estimator to data\n",
    "robust_cov = MinCovDet().fit(df.values)\n",
    "\n",
    "# compare estimators learnt from the full data set with true parameters\n",
    "emp_cov = EmpiricalCovariance().fit(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the scores for each point\n",
    "emp_mahal = emp_cov.mahalanobis(df - np.mean(df.values, 0)) ** (0.5)\n",
    "robust_mahal = robust_cov.mahalanobis(df - robust_cov.location_) ** (0.5)\n",
    "df_mahal = pd.DataFrame({'Emperical': emp_mahal, 'Robust': robust_mahal})\n",
    "\n",
    "print df_mahal.shape\n",
    "df_mahal.head(4)\n",
    "\n",
    "bins = np.arange(0., df_mahal.max().max(), 0.1)\n",
    "normed = False\n",
    "plt.hist(df_mahal['Emperical'], bins=bins, normed=normed, histtype='step', linewidth=3, label='Emperical')\n",
    "plt.hist(df_mahal['Robust'], bins=bins, normed=normed, histtype='step', linewidth=3, label='Robust')\n",
    "plt.legend()\n",
    "#plt.scatter(df_mahal['Emperical'], df_mahal['Robust'], s=10)\n",
    "df_mahal.describe(percentiles=np.arange(0.,1,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = 'Robust'\n",
    "thresh = 3. # as in Nd equivalent of 3sigma\n",
    "\n",
    "bool_ = df_mahal[col] > thresh\n",
    "idx_outliers = df_mahal[bool_].index\n",
    "idx_inliers = df_mahal[~bool_].index\n",
    "\n",
    "print len(idx_outliers), len(idx_inliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display results in 2D\n",
    "colx = df.columns[0]\n",
    "coly = df.columns[1]\n",
    "print colx, coly\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "plt.subplots_adjust(hspace=-.1, wspace=.4, top=.95, bottom=.05)\n",
    "\n",
    "# Show data set\n",
    "subfig1 = plt.subplot(3, 1, 1)\n",
    "markerSize = 50\n",
    "inlier_plot = subfig1.scatter(df_Xoutlier.loc[idx_inliers, colx], df_Xoutlier.loc[idx_inliers, coly],s=markerSize,\n",
    "                              color='black', label='training')\n",
    "outlier_plot = subfig1.scatter(df_Xoutlier.loc[idx_outliers, colx], df_Xoutlier.loc[idx_outliers, coly], s=markerSize, marker='x',\n",
    "                               color='red', label='outliers')\n",
    "#subfig1.set_xlim(subfig1.get_xlim()[0], 11.)\n",
    "subfig1.set_title(\"Mahalanobis distances of a contaminated data set:\")\n",
    "\n",
    "# Show contours of the distance functions\n",
    "xx, yy = np.meshgrid(np.linspace(plt.xlim()[0], plt.xlim()[1], 100),\n",
    "                     np.linspace(plt.ylim()[0], plt.ylim()[1], 100))\n",
    "zz = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "if df_Xoutlier.shape[1] == 2:\n",
    "    # need to update code to project Nd to 2d (as in 2d slice)\n",
    "    mahal_emp_cov = emp_cov.mahalanobis(zz) # to do so, zz needs to be updated\n",
    "    mahal_emp_cov = mahal_emp_cov.reshape(xx.shape)\n",
    "    emp_cov_contour = subfig1.contour(xx, yy, np.sqrt(mahal_emp_cov),\n",
    "                                      cmap=plt.cm.PuBu_r,\n",
    "                                      linestyles='dashed')\n",
    "\n",
    "    mahal_robust_cov = robust_cov.mahalanobis(zz)\n",
    "    mahal_robust_cov = mahal_robust_cov.reshape(xx.shape)\n",
    "    robust_contour = subfig1.contour(xx, yy, np.sqrt(mahal_robust_cov),\n",
    "                                     cmap=plt.cm.YlOrBr_r, linestyles='dotted')\n",
    "\n",
    "\n",
    "    subfig1.legend([emp_cov_contour.collections[1], robust_contour.collections[1],\n",
    "                    inlier_plot, outlier_plot],\n",
    "                   ['MLE dist', 'robust dist', 'in-liers', 'out-liers'],\n",
    "                   loc=\"upper right\", borderaxespad=0)\n",
    "plt.xticks(())\n",
    "plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Class SVM\n",
    "\n",
    "[scikit-learn module](http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html)   \n",
    "[scikit-learn tutorial](http://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html)\n",
    "\n",
    "Here we use a non-linear kernel called RBF\n",
    "\n",
    "> Strictly-speaking, the One-class SVM is not an outlier-detection method, but a novelty-detection method: its training set should not be contaminated by outliers as it may fit them. That said, outlier detection in high-dimension, or without any assumptions on the distribution of the inlying data is very challenging, and a One-class SVM gives useful results in these situations.   \n",
    "[scikit-learn outlier generic tutorial](http://scikit-learn.org/stable/modules/outlier_detection.html#one-class-svm-versus-elliptic-envelope-versus-isolation-forest-versus-lof)\n",
    "\n",
    "Here we show using the [Gaussian kernel (RBF- radial basis function)](https://en.wikipedia.org/wiki/Radial_basis_function_kernel)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "clf_svm = OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.001)\n",
    "clf_svm.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions\n",
    "\n",
    "df_prediction = pd.DataFrame(pd.Series(clf_svm.predict(df), index=df_Xoutlier.index), columns=['prediction'])\n",
    "bool_ = df_prediction['prediction'] == -1\n",
    "\n",
    "idx_outliers = df_prediction[bool_].index\n",
    "idx_inliers = df_prediction[~bool_].index\n",
    "\n",
    "print len(idx_outliers), len(idx_inliers)\n",
    "\n",
    "# plotting\n",
    "\n",
    "colx = df.columns[0]\n",
    "coly = df.columns[1]\n",
    "\n",
    "inlier_plot = plt.scatter(df.loc[idx_inliers, colx], df.loc[idx_inliers, coly],s=markerSize,\n",
    "                              color='black', label='training')\n",
    "outlier_plot = plt.scatter(df.loc[idx_outliers, colx], df.loc[idx_outliers, coly], s=markerSize, marker='x',\n",
    "                               color='red', label='outliers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest\n",
    "\n",
    " [scikit-learn module](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)  \n",
    "[scikit-learn example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_isolation_forest.html#sphx-glr-auto-examples-ensemble-plot-isolation-forest-py)\n",
    "\n",
    "Mean criticism:  `contamination` dictates the fraction of outliers assumed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_isolation = IsolationForest(verbose=0, contamination=0.1, n_estimators=100, \n",
    "                                max_samples='auto', random_state=4, n_jobs=-1) #, max_features=100) \n",
    "clf_isolation.fit(df)\n",
    "sr_inOutliers = pd.Series(clf_isolation.predict(df.values), index=df.index )\n",
    "\n",
    "sr_decisionResult = pd.Series(clf_isolation.decision_function(df), index=df.index )\n",
    "print sr_inOutliers.value_counts(dropna=False)\n",
    "\n",
    "sr_decisionResult.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [BayesSearchCV](skopt.BayesSearchCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "> If the training score is high and the validation score is low, the estimator is overfitting and otherwise it is working very well. A low training score and a high validation score is usually not possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv = 5\n",
    "scoring = None\n",
    "scores = cross_validate(clf, df_X.values, df_y['true'], scoring=scoring,\n",
    "                        cv=cv, return_train_score=True, n_jobs=-1)\n",
    "df_scores = pd.DataFrame(scores)\n",
    "df_scores.index.name = 'iteration'\n",
    "\n",
    "print df_scores.shape\n",
    "df_scores.head(4)\n",
    "\n",
    "# ======== examining scores ======== \n",
    "print \"{:0.3f} mean, {:0.3f} std Test Score\".format(df_scores['test_score'].mean(), df_scores['test_score'].var())\n",
    "print \"{:0.3f} mean, {:0.3f} std Train Score\".format(df_scores['train_score'].mean(), df_scores['train_score'].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves  \n",
    "\n",
    "scorings available:  \n",
    "['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']  \n",
    "[source scikit-learn model_evalutations](http://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `scikit-learn` Building  \n",
    "[scikit-learn - contributions](http://scikit-learn.org/stable/developers/contributing.html#contributing)  \n",
    "[Charles Finn's blog](https://ca-commercial.com/news/building-scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example case: OrdinalClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin # RegressorMixin, TransformerMixin\n",
    "from sklearn.utils.validation import (check_X_y, check_array,check_is_fitted)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "class OrdinalClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "            def __init__(base_classifier=LogisticRegression()):\n",
    "                self.base_classifier = base_classifier\n",
    "\n",
    "            def fit(self, X, y, **kwargs):\n",
    "                # Check that X and y have the correct shape:\n",
    "                X, y = check_X_y(X, y)\n",
    "\n",
    "                # Store the classes seen during the fit:\n",
    "                self.classes_ = unique_labels(y)\n",
    "\n",
    "                # Store a list of fitted binary classifiers that\n",
    "                # are initially cloned from that provided:\n",
    "                self.classifiers_ = []\n",
    "\n",
    "                # Fit the various binary classifiers and append\n",
    "                # to the list:\n",
    "                for i in range(len(self.classes_) - 1):\n",
    "                    # The algorithm goes here...\n",
    "\n",
    "                return self\n",
    "\n",
    "            def predict(X):\n",
    "                # Check that fit() has been called:\n",
    "                check_is_fitted(self, ['classes_', 'estimators_'])\n",
    "                # Input validation:\n",
    "                X = check_array(X)\n",
    "\n",
    "                # Compute the predictions using the fitted classifiers\n",
    "                # Return the result…\n",
    "                \n",
    " y_pred = OrdinalClassifier().fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example case: MSA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from mca import mca # pip install mca, from https://pypi.python.org/pypi/mca\n",
    "\n",
    "class MCA(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Multiple Correspondance Analysis (MCA).\n",
    "    Parameters\n",
    "    ----------\n",
    "    percent : float\n",
    "        The minimum variance that the retained factors are required to\n",
    "        explain (default=0.9).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, percent=0.9):\n",
    "        self.percent = percent\n",
    "        self.mca_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.mca_ = mca(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.mca_.fs_r_sup(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "[Wide and Deep](https://www.tensorflow.org/tutorials/wide_and_deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Theory\n",
    "\n",
    "[Tutorial on Graph Theory](https://www.python-course.eu/graphs_python.php)\n",
    "\n",
    "[Depth/Breadth First Search](http://eddmann.com/posts/depth-first-search-and-breadth-first-search-in-python/)\n",
    "\n",
    "[More Depth/Breadth Search](https://jeremykun.com/2013/01/22/depth-and-breadth-first-search/)\n",
    "\n",
    "[publication: An Integrated Network Modeling for Road Maps](https://link.springer.com/chapter/10.1007/978-981-10-2158-9_2)  \n",
    "\n",
    "[publication:Modeling spatial decisions with graph theory: logging roads and forest fragmentation in the Brazilian Amazon.](https://www.ncbi.nlm.nih.gov/pubmed/23495649)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# I think that this is one directional ...\n",
    "graph_ = {1: set([2, 3, 4]), 2: set([4, 5]), 3: set([5]), 4: set([]), 5: set([4]), 6: set([7]), 7:set([6]), 8:set([7])}\n",
    "graph_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dfs_recursive(graph, start, sought, visited=None):\n",
    "    \n",
    "    if start == sought:\n",
    "        return True\n",
    "    \n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    visited.add(start)\n",
    "    \n",
    "    for adjacent in (graph[start] - visited):\n",
    "        if dfs_recursive(graph, adjacent, sought, visited=visited):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "    \n",
    "dfs_recursive(graph_, 5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dfs_loop(graph, start, sought):\n",
    "    \n",
    "    #if start == sought:\n",
    "    #    return True\n",
    "    \n",
    "    visited = set()\n",
    "    \n",
    "    stack = set([start])\n",
    "    \n",
    "    while len(stack)>0:\n",
    "        node = stack.pop()\n",
    "        visited.add(node)\n",
    "        \n",
    "        if node == sought:\n",
    "             return True\n",
    "            \n",
    "        for adjacent in (graph[node] - visited):\n",
    "            stack.add(adjacent)\n",
    "        \n",
    "    return False\n",
    "    \n",
    "    \n",
    "    \n",
    "dfs_loop(graph_, 1, 3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bfs_loop(graph, start, sought):\n",
    "    \n",
    "    #if start == sought:\n",
    "    #    return True\n",
    "    \n",
    "    visited = set()\n",
    "    \n",
    "    queue = deque([start])\n",
    "    \n",
    "    while len(stack)>0:\n",
    "        node = stack.pop()\n",
    "        visited.add(node)\n",
    "        \n",
    "        if node == sought:\n",
    "             return True\n",
    "            \n",
    "        for adjacent in (graph[node] - visited):\n",
    "            queue.appendleft(adjacent)\n",
    "        \n",
    "    return False\n",
    "    \n",
    "    \n",
    "    \n",
    "dfs_loop(graph_, 8, 6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# improving quality of plots \n",
    "\n",
    "import matplotlib as mpl\n",
    "dpi = 150 # 300\n",
    "mpl.rcParams['figure.dpi']= dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subplotting in many panels\n",
    "npanels = 7 \n",
    "\n",
    "ncols = 4\n",
    "nrows = npanels / ncols + np.sum( (npanels % ncols) != 0 )\n",
    "\n",
    "width, height = 6, 8 \n",
    "plt.figure(figsize= (width*ncols, height*nrows))\n",
    "for panel in range(npanels):\n",
    "    plt.subplot(nrows, ncols, panel + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting a matrix that is sparse (mostly 0)\n",
    "# Z: sparse array (n, m)\n",
    "plt.spy(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `seaborn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seaborn \n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangular Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting for correlations\n",
    "corr = df_training[l_numerical].corr()\n",
    "corr = corr.applymap(np.abs)\n",
    "\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, center=0,annot=True, \n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df2corrTriangle(df, absolute=False, figsize=(5, 5)):\n",
    "    corr = df.corr() * 100.\n",
    "    \n",
    "    if absolute:\n",
    "        corr = corr.applymap(np.abs)\n",
    "    \n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    \n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, center=0,annot=True, \n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    \n",
    "df2corrTriangle(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Making\n",
    "\n",
    "## `folium`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import folium  # to install: pip install folium\n",
    "from folium import plugins\n",
    "\n",
    "folium.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initiate map (where and how deep, higher zoom_start is more zoomed in)\n",
    "lat_long = (35.1264416, 33.3309026)\n",
    "zoom_start = 13# higher values is more zoomed in\n",
    "\n",
    "map_ = folium.Map(location=lat_long, zoom_start=zoom_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# heatmap layer\n",
    "heatmap = plugins.HeatMap([[row['latitude'], row['longitude']] for name, row in df_geo.iterrows()], name = \"Heatmap\")\n",
    "map_.add_child(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drawing circles\n",
    "radius_miles = 50.\n",
    "kms_in_mile = 1.60934\n",
    "radius_meters = (radius_miles * kms_in_mile) * 1000.\n",
    "\n",
    "# filled circle\n",
    "folium.Circle(location=lat_long_temp, radius=radius_meters, fill_opacity=0.3, \n",
    "                    popup='some text here', color='#3186cc',fill_color='#3186cc', fill=True).add_to(map_)\n",
    "\n",
    "# many empty circles\n",
    "radius_fracs = [0.25, 0.5, 0.75, 1.]\n",
    "\n",
    "for frac in radius_fracs:\n",
    "    radius_meters_temp = radius_meters * frac\n",
    "    folium.Circle(location=lat_long_temp, radius=radius_meters_temp, fill_opacity=0.3, \n",
    "                  popup='some text here', color='#3186cc',fill_color='#3186cc', fill=False).add_to(map_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# marker layer\n",
    "feature_group_centers = folium.FeatureGroup(name='Centers')\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # we can label each label with popup \n",
    "    label = \"Center #{} \".format(idx)\n",
    "    folium.Marker([row['latitude'], row['longitude']], popup=label).add_to(feature_group_centers)\n",
    "    \n",
    "feature_group_centers.add_to(map_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T16:16:09.906075Z",
     "start_time": "2018-02-12T16:16:09.900606Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cluster marker layer\n",
    "from folium import plugins\n",
    "\n",
    "feature_group = plugins.MarkerCluster(name=\"Clustering Markers\")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    label = u\"{}\".format(idx)\n",
    "    folium.Marker([row['latitude'], row['longitude']], popup=label).add_to(feature_group)\n",
    "\n",
    "map_.add_child(feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Icons**  \n",
    "[FontAwesome gallery](https://fontawesome.com/icons?d=gallery)   \n",
    "[Details (Github)](https://github.com/lvoogdt/Leaflet.awesome-markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "icon = folium.Icon(icon='female', prefix='fa', color='green')\n",
    "folium.Marker([latitude, longitude], popup=label, icon=icon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choropleth  \n",
    "\n",
    "Requires geoJson, e.g [USA Census ZIP Codes (`tl_2010_[STATEfips]_zcta510.geojson`)](https://www.census.gov/cgi-bin/geo/shapefiles2010/main)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Step **   \n",
    "Verifying the GeoJson is displays nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geopandas # to install: pip install geopandas\n",
    "\n",
    "# loading geoJson to GeoDataFrame\n",
    "gdf_geo = geopandas.read_file(geoJson_file)\n",
    "\n",
    "# creating GeoJson object\n",
    "geojson_ = folium.GeoJson(gdf_geo) # example GeoJson used\n",
    "\n",
    "# plotting in map (uniform color)\n",
    "map_.add_child(geojson_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Defining Color Squence **   \n",
    "Second step: Adding color by metric  \n",
    "\n",
    "The metrics are converted to color by a mapping. Here I use a linear mapping, but there are other options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import branca.colormap as cm # branca should be installed with folium\n",
    "\n",
    "# choose colors\n",
    "l_colors = [\"#edf8fb\",  # greens\n",
    "\"#ccece6\",\n",
    "'#99d8c9',\n",
    "'#66c2a4',\n",
    "'#41ae76',\n",
    "\"#238b45\", \n",
    "\"#005824\"]\n",
    "\n",
    "l_colors = [  # yellow to red\n",
    "'#ffffb2',\n",
    "'#fed976',\n",
    "'#feb24c',\n",
    "'#fd8d3c',\n",
    "'#fc4e2a',\n",
    "'#e31a1c',\n",
    "'#b10026']\n",
    "\n",
    "\n",
    "# choose minimum and maximum values they rperprse\n",
    "vmin = 0\n",
    "vmax = 100\n",
    "\n",
    "colors_linear = cm.LinearColormap(l_colors, #['blue','green', 'yellow', 'red'],\n",
    "                                      vmin=vmin, vmax=vmax\n",
    "                                     )\n",
    "\n",
    "colors_linear.caption = \"my metric\"\n",
    "\n",
    "# Example colors\n",
    "print colors_linear(55) # linear interpolation - not in the original list\n",
    "print colors_linear(150) # above range get vmax\n",
    "print colors_linear(-150) # below range get vmax\n",
    "\n",
    "colors_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**metric**  \n",
    "\n",
    "Third Step: Metric in GDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# making a metric (this example is random numbers)\n",
    "gdf_geo.loc[:, 'my_metric'] = np.random.uniform(vmin, vmax, len(gdf_geo))\n",
    "\n",
    "# and creating a dictionary mapping the ID to the metric\n",
    "dict_id2metric = gdf_geo['my_metric'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Putting it all together \n",
    "# with some more folium features that might be useful\n",
    "\n",
    "fillOpacity = 0.5\n",
    "def my_style_function(feature):\n",
    "    return {\n",
    "        'fillColor': colors_linear(dict_id2metric[feature['id']]),\n",
    "        'color': 'black',\n",
    "        'weight': 2,\n",
    "        'dashArray': '0, 0',\n",
    "        'opacity': 0.7,\n",
    "        'fillOpacity': fillOpacity \n",
    "    }  \n",
    "\n",
    "def my_highlight_function(feature):\n",
    "    return {\n",
    "        'fillColor': colors_linear(dict_id2metric[feature['id']]),\n",
    "        'color': 'white',\n",
    "        'weight': 3,\n",
    "        'dashArray': '0, 0',\n",
    "        'opacity': 1.,\n",
    "        'fillOpacity': 1. \n",
    "    }\n",
    "\n",
    "\n",
    "geojson_ = folium.GeoJson(gdf_sample,\n",
    "                          style_function=my_style_function,\n",
    "                          highlight_function=my_highlight_function,\n",
    "                          name = \"ZIP Code Choropleth\", \n",
    "                          control= True,\n",
    "                          overlay= True, \n",
    "                         ) \n",
    "\n",
    "# ==== initiating map ======\n",
    "lat_long = (31.9686, -99.9018) # center of Texas\n",
    "zoom_start = 7\n",
    "map_ = folium.Map(location=lat_long, zoom_start=zoom_start)\n",
    "\n",
    "# ==== displaying the colored GEOJSON ======\n",
    "map_.add_child(geojson_)\n",
    "\n",
    "# and the color bar\n",
    "map_.add_child(colors_linear)\n",
    "\n",
    "\n",
    "map_.add_child(folium.LayerControl(position='topleft', collapsed=True, autoZIndex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer control\n",
    "map_.add_child(folium.LayerControl(collapsed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to save\n",
    "map_.save('./maps/test.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `geopy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating distances\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "great_circle(latLong[0], latLong[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for many\n",
    "df['distance_km'] = df['latlong_AB'].map(lambda x: great_circle(x[0], x[1]).km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics  \n",
    "\n",
    "Some personal notes on statistics are [here](http://bit.ly/2BA02Fr). \n",
    "\n",
    "## `scipy.stats`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pearson r\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "r, pval = pearsonr(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One way (one variable)\n",
    "\n",
    "# === in one line ===\n",
    "# assuming each sample is a list of values\n",
    "F, p = stats.f_oneway(sample1, sample2, sample3) # each sample is a list of values\n",
    "print F, p\n",
    "\n",
    "# === using pandas, step by step ===\n",
    "# df_data assumes two columns: 'treatement' (categorical), 'val' numerical values\n",
    "total_mean = df_data['val'].mean()\n",
    "treatment_size = df_data.groupby('treatment').size()\n",
    "treatment_means = df_data.groupby('treatment').mean()['val']\n",
    "\n",
    "# between group sum of squared differences\n",
    "S_b = treatment_size.dot((treatment_means - total_mean) ** 2)\n",
    "# The between-group degrees of freedom is one less than the number of groups\n",
    "f_b = len(treatment_size) - 1\n",
    "# between-group mean square value\n",
    "MS_b = S_b * 1. / f_b\n",
    "\n",
    "# within group sum of squares\n",
    "S_w = 0\n",
    "for treatment, df_treatement in df_temp.groupby('treatment'):\n",
    "    S_w += ((df_treatement['val'] - df_treatement['val'].mean()) ** 2).sum()\n",
    "# The within-group degrees of freedom is    \n",
    "f_w = len(df_temp) - len(treatment_size)\n",
    "#Within-group mean square value is\n",
    "MS_w = S_w * 1. / f_w\n",
    "\n",
    "F = MS_b * 1. / MS_w\n",
    "\n",
    "# To reject the null hypothesis we check if the obtained F-value is above the critical value \n",
    "# for rejecting the null hypothesis. We could look it up in a F-value table based on \n",
    "# the DFwithin and DFbetween. However, there is a method in SciPy for obtaining a p-value.\n",
    "p = stats.f.sf(F, f_b, f_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sqlalchemy\n",
    "[site](https://www.sqlalchemy.org/)  \n",
    "[docs](http://docs.sqlalchemy.org/en/latest/intro.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlalchemy # pip install SQLAlchemy\n",
    "\n",
    "# Engine creation\n",
    "engine = sqlalchemy.engine.create_engine(connect) # see below how to creat connect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connection creation\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# executing\n",
    "query_ = conn.execute(\"SELECT * FROM users LIMIT 40\")\n",
    "\n",
    "# storing in dataframe\n",
    "df_data = pd.DataFrame(query_.fetchall(), columns=query_.keys())\n",
    "\n",
    "# closing connection\n",
    "conn.close()\n",
    "\n",
    "# closing engine\n",
    "engine.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `connect` MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = '<user_name>'\n",
    "password = '<password, if required>'\n",
    "host = '<some address, or some SITE.com>'\n",
    "dbname = '<database name>'\n",
    "port = 3306 # for example\n",
    "\n",
    "connect = \"mysql+mysqldb://{}:{}@{}/{}\".format(user, password, host, dbname) # no port "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib, json, time\n",
    "\n",
    "def GeocodeAPI(address, key='', delay=5):\n",
    "    base = r'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "    \n",
    "    addP = 'address=' + address.replace(' ', '+')\n",
    "    GeoUrl = base + addP + '&key=' + key\n",
    "\n",
    "    response = urllib.urlopen(GeoUrl)\n",
    "    jsonRaw = response.read()\n",
    "    jsonData = json.loads(jsonRaw)\n",
    "\n",
    "    if jsonData['status'] == 'OK':\n",
    "        resu = jsonData['results'][0]\n",
    "        finList = [resu['geometry']['location']['lat'], resu['geometry']['location']['lng']]\n",
    "    else:\n",
    "        finList = [None,None,None]\n",
    "\n",
    "    time.sleep(delay)\n",
    "\n",
    "    return finList\n",
    "\n",
    "key = \"<your key>\" #from: https://developers.google.com/maps/documentation/javascript/get-api-key#quick-guide-to-getting-a-key\n",
    "# credentials may be found here:  https://console.developers.google.com/apis/credentials\n",
    "GeocodeAPI(address=\"4 Washington Square New York, NY\", key=key, delay=1)\n",
    "# yields 40.72739929999999, -73.9971446 (latitude, longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run multiple times\n",
    "\n",
    "out = [GeocodeAPI(address=address, key=key, delay=1) for address in df['address_full']]\n",
    "# Turn into separate columns\n",
    "df_locations['latitude_google'], df_locations['longitude_google'] = map(list, zip(*out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `recordlinkage` \n",
    "[docs](http://recordlinkage.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import recordlinkage # pip install recordlinkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## crontab  \n",
    "[reference](https://crontab.guru/)  \n",
    "\n",
    "edit  \n",
    "`> crontab -e`   \n",
    "\n",
    "Download everyday at 23:37\n",
    "\n",
    "> 37 23 * * * scp -r /from/. /home/destination/."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "434px",
    "left": "0px",
    "right": "1097.1px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
